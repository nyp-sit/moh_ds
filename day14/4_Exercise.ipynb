{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_2jIVkz_ciq"
   },
   "source": [
    "# Exercise 1 - Topic Modeling\n",
    "\n",
    "Let us now put what we have learnt in the past two lessons to use. We will perform anomaly detection and topic modeling on the procurement dataset (procurementdata.csv). We will use the tender_description column as well as the agency column for performing these analyses.\n",
    "\n",
    "\n",
    "Let us start with topic modeling. We will first load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1lGdct1McNL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Bsph3hmMcNO",
    "outputId": "86831c80-2f2d-42cc-e468-a0d127d7cd5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = 'datasets/procurementdata.csv'\n",
    "\n",
    "# Load up the data from the training CSV file.\n",
    "#\n",
    "print (\"Loading data...\")\n",
    "data_df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "zjOvu6-5McNP",
    "outputId": "51d0d508-ec97-4f03-fe26-0280535dbf88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbTP7jwLcstr"
   },
   "source": [
    "We want only the two relevant columns. Let us proceed to do the preprocessing as we have learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "rTCw2QD8McNP",
    "outputId": "d7db619c-3cec-410b-bc03-1114a5e17710",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tender=data_df['tender_description']\n",
    "agency=data_df['agency']\n",
    "dataset=tender + ' ' + agency\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF0Tqm7eMcNP",
    "outputId": "b5d509a0-3cad-416f-e8b1-7a226b79b5e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#only need to do once\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuBvVtP0McNQ",
    "outputId": "c630aab8-8e02-4a4b-ac21-4d57379487bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "dataset.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLkTmIFVMcNR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeLDrsZIMcNS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "\n",
    "    punc_free = ''.join([ch for ch in doc.lower() if ch not in exclude])\n",
    "    stop_free = ' '.join([i for i in punc_free.split() if i not in stop])\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in stop_free.split())\n",
    "    #stemmed = ' '.join(stemmer.stem(word) for word in normalized.split())\n",
    "    return normalized\n",
    "\n",
    "dataset2=dataset.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxFvE7Tfc2pP"
   },
   "source": [
    "Due to the way gensim processes the data, we need to split the string into individual words as below. We can then proceed as in the Practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGE8TFXkMcNT",
    "outputId": "bb3418d0-3f3b-4514-8f73-ee888ba2ba37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset3=dataset2.values.tolist()\n",
    "processed_docs = [doc.split() for doc in dataset3]\n",
    "print(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1GtnFx7McNU",
    "outputId": "316e794b-58d8-46c0-cde8-394626f89ca6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXyJuEb9McNU",
    "outputId": "6b1b3af6-6b70-48cd-fcbc-588e6a181a26",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bows = [dictionary.doc2bow(processed_doc) for processed_doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbnYDkT6McNV",
    "outputId": "f00992cc-67db-4624-c279-5b0ea9e5f5f0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bows[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJ8WVcQ0McNV",
    "outputId": "cfcc6cab-1162-47d1-89b1-3e7dd9cd5859",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(bows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2M4qdcXdDkQ"
   },
   "source": [
    "Let us now train the LDA model for five topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH46DpL7McNW",
    "outputId": "4c83a485-0afb-440d-fff4-b22bc59809fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 250\n",
    "passes = 20\n",
    "iterations = 50\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "id2word = dictionary\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "ldamodel = LdaModel(\n",
    "    corpus=bows,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3w_3-L8McNX",
    "outputId": "4814a14b-1aae-4289-9759-42ef341f8925",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldCDqq3gMcNX",
    "outputId": "cf49b8fb-521c-4914-9b1a-b0aa99eed132",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nFile name and its corresponding topic id with probability:')\n",
    "dic_topic_doc = {}\n",
    "for index, doc in enumerate(processed_docs):\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    #get topic distribution of the ldamodel\n",
    "    t = ldamodel.get_document_topics(bow)\n",
    "    #sort the probability value in descending order to extract the top contributing topic id\n",
    "    sorted_t = sorted(t, key=lambda x: x[1], reverse=True)\n",
    "    #print only the filename\n",
    "    print(index,sorted_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxxkvX-adKa2"
   },
   "source": [
    "We can calculate the perplexity and coherence as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-y_-x-cMcNY",
    "outputId": "8b876ea9-f48a-47e7-d07b-8ae0b08d9a96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "perplexity = ldamodel.log_perplexity(bows)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1a6cuZ2McNZ",
    "outputId": "4e03f895-82ea-4946-d15c-5cc49c48c29e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "lda_coherence = CoherenceModel(model=ldamodel,\n",
    "                               texts=processed_docs,\n",
    "                               dictionary=dictionary,\n",
    "                               coherence='c_v')\n",
    "coherence_score = lda_coherence.get_coherence()\n",
    "print(coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFdS-JZHe3d1"
   },
   "source": [
    "As before we can try to determine the best number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwnpObztej9x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, corpus, texts, limit, start=2, step=3):\n",
    "\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    topics_num = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        np.random.seed(10)\n",
    "        ldamodel = LdaModel(\n",
    "            corpus=bows,\n",
    "            id2word=id2word,\n",
    "            chunksize=250,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            iterations=50,\n",
    "            num_topics=num_topics,\n",
    "            passes=20,\n",
    "            eval_every=eval_every\n",
    "        )\n",
    "        model_list.append(ldamodel)\n",
    "        coherencemodel = CoherenceModel(model=ldamodel, texts=texts,\n",
    "                                       dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        perplexity_values.append(ldamodel.log_perplexity(corpus))\n",
    "        topics_num.append(num_topics)\n",
    "\n",
    "    return model_list, coherence_values, perplexity_values, topics_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyOYvDLRfYB7",
    "outputId": "79fba062-918d-4950-e08e-44ad64a69204",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# search through k-topics in steps\n",
    "start=1; limit=7; step=1;\n",
    "#start=5; limit=50; step=5;\n",
    "\n",
    "model_list, coherence_values, perplexity_values, topics_num = compute_coherence_values(id2word,\n",
    "                                                                           corpus=bows,\n",
    "                                                                           texts=processed_docs,\n",
    "                                                                           start=start, limit=limit, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "B5mJuVdMhM0K",
    "outputId": "c1a7cba3-8ff2-4993-bf32-bf4efc51bd4a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Show Perplexity and Coherence graph\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(start, limit, step)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Num Topics')\n",
    "ax1.set_ylabel('Perplexity Score', color=color)\n",
    "ax1.plot(x, perplexity_values, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx() #instantiate a second axes that share the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Coherence Score', color=color)\n",
    "ax2.plot(x, coherence_values, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout() # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-MR5R4HdSU2"
   },
   "source": [
    "And visualize using pyLDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "_t9x-k3oMcNa",
    "outputId": "ccb335eb-9bd9-4411-f502-222cd04c0636",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_data = gensimvis.prepare(ldamodel, bows, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j1aqBoZMcNa"
   },
   "source": [
    "# Exercise 2 - Anomaly Detection\n",
    "\n",
    "Let us now proceed with anomaly detection using isolation forest. Depending on time available, we can also randomly take 10000 samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u1hm3fWhi3i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset3=dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayH3GBX_NK1s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset3=dataset2.sample(10000, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJk-e2CjYr2k"
   },
   "source": [
    "We will use the CountVectorizer introduced in this lesson for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxN0tkFeMcNb",
    "outputId": "91f9bf30-3000-4f9a-eab7-4e418b2985f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(dataset3)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "jFtgjwQSMcNc",
    "outputId": "b6b58e3e-b0b5-4a4a-c227-03547258b621",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame(doc_vec.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPxJR7QFY3GT"
   },
   "source": [
    "Let us now use Isolation Forest to detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "ACACk_S-McNc",
    "outputId": "014d2ac7-9e18-403d-d4e9-3132901ee8e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "forest = IsolationForest(random_state=0)\n",
    "forest.fit(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bow=df_bow.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Id1Rdy0McNd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "scores = forest.score_samples(df_bow)\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "EQv6BYacMcNd",
    "outputId": "faed9fc3-c233-4feb-cf2b-1836d2db02dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(scores, bins=50)\n",
    "plt.ylabel('Number', fontsize=15)\n",
    "plt.xlabel('Anomaly score', fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "e6jHbnqtMcNe",
    "outputId": "ab344f0c-5435-4af5-957a-8cc3f50298e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_n_outliers = 5\n",
    "top_n_outlier_indices = np.argpartition(scores, top_n_outliers)[:top_n_outliers].tolist()\n",
    "top_outlier_features = df_bow.iloc[top_n_outlier_indices, :]\n",
    "top_outlier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cik0_pPlZP9l"
   },
   "source": [
    "The top anomalies found are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYuSmlhLMcNe",
    "outputId": "1a7f3011-f138-41ce-f170-813fa6ab626b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset3.iloc[1246])\n",
    "print(dataset3.iloc[7806])\n",
    "print(dataset3.iloc[13550])\n",
    "print(dataset3.iloc[2249])\n",
    "print(dataset3.iloc[13882])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP4Jtnb6ZgLt"
   },
   "source": [
    "We can also try to use SHAP to gain some understanding about how the model generates the output. Bear in mind that for this dataset, due to the very large number of features, it is still not easy to interpret the model. But we can do this as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fj1pEVRQXKX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install shap\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(forest)\n",
    "shap_values = explainer.shap_values(df_bow)\n",
    "features = df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PoJal-caAik"
   },
   "source": [
    "Let's see if we can gain any insight about the factors affecting the most significant outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ze_k7K2DQgMN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "shap.initjs\n",
    "\n",
    "dis=shap.force_plot(explainer.expected_value, shap_values[1246, :], features.iloc[1246, :],matplotlib=False)\n",
    "shap_html = f\"{shap.getjs()}{dis.html()}\"\n",
    "\n",
    "with open(\"shap_full.html\", \"w\", encoding='utf8') as file:\n",
    "    file.write(shap_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_gOch2IXGQn"
   },
   "source": [
    "From the shap summary plot below, we see that the most important word is association followed by year, etc. We also see that larger counts of these words tend to reduce the score making it more likely to be anomaly.\n",
    "\n",
    "Discussion: Although there is some trend in the data, do note the nature of this dataset, as well as the preprocessing that we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "id": "yWi8wSDxMcNg",
    "outputId": "41adad02-4687-4662-d29f-c747fde494de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbkwdyqETtUi"
   },
   "source": [
    "Let's take a look at the shap dependence plot for 'association'. As expected, for the word 'association', the higher the occurence, the more it lowers the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "jsEvTZF8McNg",
    "outputId": "53677183-a7e5-4ae9-a2d2-f06f649f5af4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    " 'association',\n",
    " shap_values,\n",
    " features,\n",
    " interaction_index=None,\n",
    " xmax='percentile(99)' #upper bound of plots x-axis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDcqIPfCcJo4"
   },
   "source": [
    "We can also look at the interaction between two features , for example 'association' and 'year'. It looks like 'year' does not have much interaction with 'association' that could affect the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "SM4y5dCaTYvS",
    "outputId": "3d6e5e40-6fd1-4ad0-a8df-e52e7044e0e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    " 'association',\n",
    " shap_values,\n",
    " features,\n",
    " interaction_index='year',\n",
    " xmax='percentile(99)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc602e7e44b83c3583925fd77ac766cf87c9915ce53c5dc0c705dc2e8f6b010c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

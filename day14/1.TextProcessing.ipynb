{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing using Python\n",
    "Python language is commonly used for text preprocessing purpose. It has suitable libraries built that we can use on the python script, such as, nltk (http://www.nltk.org/ ). \n",
    "\n",
    "We will go through a sample program to illustrate the various processing steps that can be done using a Python program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries and download the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#only need to do once\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in file for analysis\n",
    "This file must be placed in the same directory as this python file.\n",
    "Just change the name of the file to analyse other files. The code reads in the first line of the file and saves it to 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'datasets/hotel2.txt'\n",
    "fp = open(filename, 'r', encoding='UTF8')\n",
    "text = fp.readline()\n",
    "print(text)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also read the entire file and obtain the first entry as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp = open(filename, 'r', encoding='UTF8')\n",
    "text2 = fp.read()\n",
    "#print(text2)\n",
    "fp.close()\n",
    "\n",
    "text3 = text2.split('\\n')\n",
    "print(text3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Run the code below to tokenize the text, and to analyze the content of text in terms of the number of sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print('number of sentences: '+str(len(sentences)))\n",
    "\n",
    "#word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "words1 = word_tokenize(text) \n",
    "words2 = text.split()\n",
    "print('number of words1: '+str(len(words1)))\n",
    "print('first 80 words1 '+str(words1[:80]))\n",
    "print('number of words2: '+str(len(words2)))\n",
    "print('first 80 words2 '+str(words2[:80]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Examine the output above and describe the difference between words1 and words2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "Normalise each of the words to lowercase and remove some of the special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lowercase and remove punctuation\n",
    "processed1 = []\n",
    "for w in words1:\n",
    "    #processed.append(re.sub(r'([^\\s\\w]|_)+', '', w).lower())  #will clean all punctuations and numbers\n",
    "    processed1.append(re.sub('[-(),.]', '', w).lower())   #keep word like i'd\n",
    "\n",
    "processed2 = []\n",
    "for w in words2:\n",
    "    #processed.append(re.sub(r'([^\\s\\w]|_)+', '', w).lower())  #will clean all punctuations and numbers\n",
    "    processed2.append(re.sub('[-(),.]', '', w).lower())   #keep word like i'd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to print the first 10 words in processed1 and processed2\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "\n",
    "```\n",
    "print('first 10 processed1 '+str(processed1[:10]))\n",
    "print('first 10 processed2 '+str(processed2[:10]))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clean the empty strings as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed1 = list(filter(None, processed1))\n",
    "processed2 = list(filter(None, processed2))\n",
    "print(len(processed1))\n",
    "print(len(processed2))\n",
    "\n",
    "print(processed1)\n",
    "print(processed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords\n",
    "Use the stopwords from nltk to remove stopwords and store the list of words in clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "clean_words = processed2[:]\n",
    "sw = stopwords.words('english')\n",
    "print(sw)\n",
    "#sw.append('')  # include blank so that it will be removed\n",
    "for word in processed2:\n",
    "    if word in sw:\n",
    "        clean_words.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the content of clean_words by printing the list of words with its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('---Print list of words with frequency (after normalisation and stop words removal)---')\n",
    "#count word frequency\n",
    "freq = nltk.FreqDist(clean_words)\n",
    "i = 0\n",
    "top5original = []\n",
    "#display items in desc order\n",
    "for key,val in freq.most_common():\n",
    "    print(str(key) + \":\" + str(val))\n",
    "    # get the first 5 top frequency words\n",
    "    if i < 5:\n",
    "        top5original.append(str(key))\n",
    "        i += 1\n",
    "print()\n",
    "print('---Print top 5 words from the file ---')\n",
    "print('top 5 :' + str(top5original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Porter\n",
    "Do porter stemming and store the new list inside stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#stemming  - english\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('working'))\n",
    "\n",
    "#add in stemming - check bed, sound\n",
    "stemmed = []\n",
    "for c in clean_words:\n",
    "    stemmed.append(stemmer.stem(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Write some code to print out the top 5 stemmed list of words\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "\n",
    "```\n",
    "sfreq = nltk.FreqDist(stemmed)\n",
    "i = 0\n",
    "top5 = []\n",
    "#display items in desc order\n",
    "for key,val in sfreq.most_common():\n",
    "    print(str(key) + \":\" + str(val))\n",
    "    # get the first 5 top frequency words\n",
    "    if i < 5:\n",
    "        top5.append(str(key))\n",
    "        i += 1\n",
    "print()\n",
    "print('---Print top 5 stemmed words ---')\n",
    "print('top 5 :'+str(top5))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse top 5 words by retrieving their bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create bigram and search for top 5 words\n",
    "bigrams = list(nltk.ngrams(processed2,2))\n",
    "\n",
    "print(bigrams[:10])\n",
    "\n",
    "print()\n",
    "print('---Print top 5 stemmed words and its bigrams ---')\n",
    "\n",
    "for k in top5:\n",
    "    print(k + \": \"+str([s for s in bigrams if any(k in x for x in s)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Try writing code to print the top 5 bigrams after normalisation, removal of stop words and stemming\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "\n",
    "```\n",
    "bigrams2=list(nltk.ngrams(stemmed,2))\n",
    "print()\n",
    "print('---Print list of bigrams with frequency (after normalisation, stop words removal and stemming)---')\n",
    "#count word frequency\n",
    "freq = nltk.FreqDist(bigrams2)\n",
    "i = 0\n",
    "top5original = []\n",
    "#display items in desc order\n",
    "for key,val in freq.most_common():\n",
    "    print(str(key) + \":\" + str(val))\n",
    "    # get the first 5 top frequency words\n",
    "    if i < 5:\n",
    "        top5original.append(str(key))\n",
    "        i += 1\n",
    "print()\n",
    "print('---Print top 5 bigrams ---')\n",
    "print('top 5 :' + str(top5original))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use lemmatization instead of stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write some code to perform lemmatization and to print the top 5 lemmatized list of words. Examine if there are any differences with stemming.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "\n",
    "```\n",
    "lemmatized = []\n",
    "for c in clean_words:\n",
    "    lemmatized.append(lemmatizer.lemmatize(c))\n",
    "\n",
    "sfreq = nltk.FreqDist(lemmatized)\n",
    "i = 0\n",
    "top5 = []\n",
    "#display items in desc order\n",
    "for key,val in sfreq.most_common():\n",
    "    print(str(key) + \":\" + str(val))\n",
    "    # get the first 5 top frequency words\n",
    "    if i < 5:\n",
    "        top5.append(str(key))\n",
    "        i += 1\n",
    "print()\n",
    "print('---Print top 5 lemmatized words ---')\n",
    "print('top 5 :'+str(top5))\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation \n",
    "First, we will use the bag of words method to create the frequency matrix from the stemmed data. We can do this using scikitlearn's CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create data in dataframe for analysis\n",
    "data1 = \" \".join(stemmed)\n",
    "df1 = pd.DataFrame({'data': [data1]})\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(df1.iloc[0])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame(doc_vec.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the term frequency inverse document frequency matrix using scikitlearn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(df1.iloc[0])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(doc_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame(doc_vec.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Word2Vec\n",
    "Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. \n",
    "\n",
    "We will go through a sample program to generate word vectors using Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first process the entire text file instead of a single row. You can try to write the code for this\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "\n",
    "```\n",
    "\n",
    "filename = 'datasets/hotel2.txt'\n",
    "fp = open(filename, 'r', encoding='UTF8')\n",
    "text = fp.read()\n",
    "print(text)\n",
    "fp.close()\n",
    "\n",
    "data = [] \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(text): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in i.split():\n",
    "        temp.append(re.sub('[-(),.]', '', j).lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "#remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw.append('')  # include blank so that it will be removed\n",
    "sw.append('the')\n",
    "\n",
    "clean_data = []\n",
    "for sentence in data:\n",
    "    temp = sentence\n",
    "    for word in sentence:\n",
    "        if word in sw:\n",
    "            temp.remove(word)\n",
    "    \n",
    "    clean_data.append(temp)   \n",
    "\n",
    "print(clean_data)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec Model\n",
    "Instantiate Word2Vec and pass the reviews that we read in the previous step. Each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary.\n",
    "\n",
    "The results is a learned vector, also known as the embeddings. You can think of these embeddings as some features that describe the target word. For example, the word king may be described by the gender, age, the type of people the king associates with, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "# build vocabulary and train model\n",
    "# Create CBOW model \n",
    "# size: size of dense vector to represent each token or word \n",
    "# window: maximum distance between target word and its neighboring word\n",
    "# min_count: minimium frequency count of words\n",
    "# iter: number of iterations (epochs)\n",
    "model = gensim.models.Word2Vec(clean_data, min_count = 3,  \n",
    "                              vector_size = 100, window = 5) # sg =1 for skip-gram\n",
    "\n",
    "model.save('mymodel')\n",
    "new_model = gensim.models.Word2Vec.load('mymodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the **most_similar** function and provide a word, it will return the top 10 similar words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1 = \"service\"\n",
    "model.wv.most_similar (w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Word2Vec to compute similarity between two words in the vocabulary by using the **similarity** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#excellent is highly similar to best\n",
    "model.wv.similarity(w1=\"best\", w2=\"excellent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#worst is dissimilar to best\n",
    "model.wv.similarity(w1=\"best\", w2=\"worst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the 100-dimensional vector created for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.wv.get_vector('excellent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Try to perform BOW or TFIDF on the entire dataset. The code to convert the dataset to a dataframe has been written for you. Try to limit the vocabulary size to the 1000 most frequent by using max_features\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "\n",
    "```\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=1000)\n",
    "doc_vec = vectorizer.fit_transform(clean_data_2.iloc[:,0])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(doc_vec)\n",
    "\n",
    "\n",
    "df_bow = pd.DataFrame(doc_vec.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df_bow.head()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_data_2=pd.DataFrame(columns=['data'])\n",
    "for i in clean_data:\n",
    "    data1 = \" \".join(i)\n",
    "    a=pd.DataFrame([data1],columns=['data'])\n",
    "    clean_data_2=pd.concat([clean_data_2,a])\n",
    "print(clean_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources 1 - Synonyms\n",
    "\n",
    "Sample code to find synonyms in WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#synonyms definition\n",
    "#Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet.\n",
    "#The code below gives the definition for NLP\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"NLP\")\n",
    "print(syn[0].definition())\n",
    "\n",
    "#synonymous words\n",
    "#The code below gives the synonyms for computer\n",
    "syn=wordnet.synsets(\"Computer\")\n",
    "print(syn)\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('Computer'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc602e7e44b83c3583925fd77ac766cf87c9915ce53c5dc0c705dc2e8f6b010c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

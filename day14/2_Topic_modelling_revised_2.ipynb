{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7748ca",
   "metadata": {},
   "source": [
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0a7a7-fc53-4be7-971f-6971fea58243",
   "metadata": {
    "id": "70b0a7a7-fc53-4be7-971f-6971fea58243"
   },
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this exercise, we we will learn how to use topic modeling to find 'latent' topics of a large corpus of text.\n",
    "\n",
    "At the end of the session, you will learn:\n",
    "- how to use the popular *gensim* library to create topic models\n",
    "- the importance of choosing the correct granularity of entities for calculating topic models\n",
    "- to experiment with many parameters to find the optimal topic model\n",
    "- to evaluate the quality of the resulting topic models by quantitative methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1073a-e844-4297-afc4-bdeb1212dcb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3d1073a-e844-4297-afc4-bdeb1212dcb2",
    "outputId": "9f7f79c4-4e67-436b-9612-9246ab2f63b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "step 1. import necessary libraries\n",
    "'''\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "import os\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eed95a-ff06-43b0-b9c8-a71bea1a9989",
   "metadata": {
    "id": "77eed95a-ff06-43b0-b9c8-a71bea1a9989",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c1b74-1922-49a0-b49b-59e796cb15e9",
   "metadata": {
    "id": "fc8c1b74-1922-49a0-b49b-59e796cb15e9"
   },
   "source": [
    "## Getting the Data\n",
    "\n",
    "We have a few hundred news articles, and we are interested in knowing what are the 'topics' the different news articles are taling about.  To build a topic model, we first need to build our corpus. We will loop through our data directory and read all the text files and append them to the corpus. If you need to unzip the data file in Colab, you can use !unzip file.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54f363-49a2-4fd6-a647-0b751c128498",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c54f363-49a2-4fd6-a647-0b751c128498",
    "outputId": "2bc2339a-5230-4ebd-8d24-7e63526c6ffe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "step 2. read in files (from directory) for analysis\n",
    "'''\n",
    "\n",
    "# r is the raw sting literals so that windows path slash won't create problem\n",
    "data_folder = Path(r'datasets/news')\n",
    "\n",
    "# read each file from the directory into an array and name it corpus\n",
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for filename in data_folder.iterdir():\n",
    "    if os.path.isfile(filename):\n",
    "        fp = open(str(filename), 'r', encoding='iso8859_2')\n",
    "        corpus.append(fp.read())\n",
    "\n",
    "        #keep the filename for later use\n",
    "        filenames.append(filename.name)\n",
    "        fp.close()\n",
    "\n",
    "print(corpus.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc9a51-ca6d-4e04-90d0-5957d577c257",
   "metadata": {
    "id": "fedc9a51-ca6d-4e04-90d0-5957d577c257"
   },
   "source": [
    "## Preprocess the Text Data\n",
    "\n",
    "We will need to preprocess the text data before we can use it to train our model. Typical preprocessing steps include: tokenization, normalization, stopword removal, stemming/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca7bea-0d61-408e-8159-e61e2ecde3f4",
   "metadata": {
    "id": "82ca7bea-0d61-408e-8159-e61e2ecde3f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Use the spacy stopwords instead\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "# stop = set(stopwords)\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c357a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e66c357a",
    "outputId": "89635a66-a15c-47ee-ea0b-e8b65e4165bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011c9d8-4bab-49db-96d4-9805689f58ca",
   "metadata": {
    "id": "e011c9d8-4bab-49db-96d4-9805689f58ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "\n",
    "    punc_free = ''.join([ch for ch in doc.lower() if ch not in exclude])\n",
    "    stop_free = ' '.join([i for i in punc_free.split() if i not in stop])\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in stop_free.split())\n",
    "    #stemmed = ' '.join(stemmer.stem(word) for word in normalized.split())\n",
    "    return normalized\n",
    "\n",
    "processed_docs = [preprocess(doc).split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab73394",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ab73394",
    "outputId": "27f51074-f6cc-4688-d20e-507b72f9e368",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd089027",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd089027",
    "outputId": "b81e3086-d482-4f31-a6a3-05c844845d8b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafec412-4c2c-4a0a-9674-cae49fdbe3c8",
   "metadata": {
    "id": "aafec412-4c2c-4a0a-9674-cae49fdbe3c8"
   },
   "source": [
    "Once we have the preprocessed list of tokens, we can then create a Dictionary, which is basically a word to integer id mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a1585-1286-4883-a31f-70ebd362c6ab",
   "metadata": {
    "id": "f67a1585-1286-4883-a31f-70ebd362c6ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(processed_docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "#dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc54612-d15c-45d1-80d1-b8bd6c25fae9",
   "metadata": {
    "id": "4dc54612-d15c-45d1-80d1-b8bd6c25fae9"
   },
   "source": [
    "Finally, we transform the documents to a vectorized form (aka. document term matrix).  Here we convert each doc to bag-of-words (which is just frequency count of each word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828af1bf-d21b-42c3-bfe4-143dcee396c0",
   "metadata": {
    "id": "828af1bf-d21b-42c3-bfe4-143dcee396c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bows = [dictionary.doc2bow(processed_doc) for processed_doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26acc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e26acc2",
    "outputId": "2bc4c10f-a719-49c8-9496-3c8489576658",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bows[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7bc9e-08c2-46d5-91d6-6a30a99adf18",
   "metadata": {
    "id": "e8b7bc9e-08c2-46d5-91d6-6a30a99adf18"
   },
   "source": [
    "Let’s see how many tokens and documents we have to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe72a8-7f7e-466f-9721-9d197af57154",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70fe72a8-7f7e-466f-9721-9d197af57154",
    "outputId": "93c26eb5-7d6d-4a7a-9439-0ba1bb89feb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(bows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de8e77-2b3e-4e9a-83f6-e5991a0ded90",
   "metadata": {
    "id": "31de8e77-2b3e-4e9a-83f6-e5991a0ded90"
   },
   "source": [
    "## Train LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617dbec9-ff26-4d69-8322-41af1598726c",
   "metadata": {
    "id": "617dbec9-ff26-4d69-8322-41af1598726c"
   },
   "source": [
    "We are ready to train the LDA model. We will first discuss how to set some of the training parameters.\n",
    "\n",
    "First question: how many topics do I need? There is really no easy answer for this, it will depend on both your data and your application. Here we use 5, but it can be any of your choice. We will see later how we can use perplexity or coherence to decide on the 'optimal' number of topics.\n",
    "\n",
    "`chunksize` controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
    "\n",
    "`passes` controls how often we train the model on the entire corpus. Another word for passes might be “epochs”.\n",
    "\n",
    "`iterations` is somewhat technical, but essentially it controls how often we repeat the algorithm. It is important to set the number of “passes” and “iterations” high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acab23a-9891-4f89-b522-133ff951a52d",
   "metadata": {
    "id": "3acab23a-9891-4f89-b522-133ff951a52d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 250\n",
    "passes = 20\n",
    "iterations = 50\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "id2word = dictionary\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "ldamodel = LdaModel(\n",
    "    corpus=bows,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bc692",
   "metadata": {
    "id": "d38bc692"
   },
   "source": [
    "The above LDA model is built with 5 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9571d9-e3c7-414e-8157-de81c9318510",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a9571d9-e3c7-414e-8157-de81c9318510",
    "outputId": "7119e89f-2f58-41c7-83b6-c28b85d1ff68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad927c24",
   "metadata": {
    "id": "ad927c24"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "367cbad0",
   "metadata": {
    "id": "367cbad0"
   },
   "source": [
    "We can also obtain all the documents and their topic ids with corresponding probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc462f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19fc462f",
    "outputId": "cc66063a-d73e-49e5-c64a-0681c36fda87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nFile name and its corresponding topic id with probability:')\n",
    "dic_topic_doc = {}\n",
    "for index, doc in enumerate(processed_docs):\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    #get topic distribution of the ldamodel\n",
    "    t = ldamodel.get_document_topics(bow)\n",
    "    #sort the probability value in descending order to extract the top contributing topic id\n",
    "    sorted_t = sorted(t, key=lambda x: x[1], reverse=True)\n",
    "    #print only the filename\n",
    "    print(filenames[index],sorted_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b84c87-a414-4f5f-8723-6b0db184ba47",
   "metadata": {
    "id": "34b84c87-a414-4f5f-8723-6b0db184ba47"
   },
   "source": [
    "One way to measure our model is the perplexity score. The lower the perplexity the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e5db3-0de2-4788-bdc8-435f5bbf7f07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c4e5db3-0de2-4788-bdc8-435f5bbf7f07",
    "outputId": "5373e85c-042f-4d34-b0fe-049465118ff2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "perplexity = ldamodel.log_perplexity(bows)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d5fcd-38a9-4700-b975-f59325a90044",
   "metadata": {
    "id": "6f3d5fcd-38a9-4700-b975-f59325a90044"
   },
   "source": [
    "We can also use coherence score to measure our model. The higher the coherence score the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667993c-d222-4336-b6e4-1b8ebd6f1254",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5667993c-d222-4336-b6e4-1b8ebd6f1254",
    "outputId": "5b6f2479-e087-453d-9f88-d3339ea35e60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "lda_coherence = CoherenceModel(model=ldamodel,\n",
    "                               texts=processed_docs,\n",
    "                               dictionary=dictionary,\n",
    "                               coherence='c_v')\n",
    "coherence_score = lda_coherence.get_coherence()\n",
    "print(coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df0eee",
   "metadata": {
    "id": "e2df0eee"
   },
   "source": [
    "# Optimal number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf9683-bff7-4240-9d19-66ea0396ee09",
   "metadata": {
    "id": "ccaf9683-bff7-4240-9d19-66ea0396ee09"
   },
   "source": [
    "We can find the number of topics that gives us the lowest perplexity and coherence score using the following codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4d1e0-a962-4876-b715-f869210d571b",
   "metadata": {
    "id": "6df4d1e0-a962-4876-b715-f869210d571b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, corpus, texts, limit, start=2, step=3):\n",
    "\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    topics_num = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        np.random.seed(10)\n",
    "        ldamodel = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            chunksize=250,\n",
    "            alpha='auto',\n",
    "            eta='auto',\n",
    "            iterations=50,\n",
    "            num_topics=num_topics,\n",
    "            passes=20,\n",
    "            eval_every=eval_every\n",
    "        )\n",
    "        model_list.append(ldamodel)\n",
    "        coherencemodel = CoherenceModel(model=ldamodel, texts=texts,\n",
    "                                       dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        perplexity_values.append(ldamodel.log_perplexity(corpus))\n",
    "        topics_num.append(num_topics)\n",
    "\n",
    "    return model_list, coherence_values, perplexity_values, topics_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29a069-ce30-4c9a-98ff-9b3411286567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf29a069-ce30-4c9a-98ff-9b3411286567",
    "outputId": "c7e48372-cf53-467d-9034-19a0dcfc2675",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search through k-topics in steps\n",
    "start=1; limit=7; step=1;\n",
    "#start=5; limit=50; step=5;\n",
    "\n",
    "model_list, coherence_values, perplexity_values, topics_num = compute_coherence_values(id2word,\n",
    "                                                                           corpus=bows,\n",
    "                                                                           texts=processed_docs,\n",
    "                                                                           start=start, limit=limit, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e821e8d-7ff0-4d3c-acda-df012c23f45e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "4e821e8d-7ff0-4d3c-acda-df012c23f45e",
    "outputId": "81025964-68f6-4826-9e9c-9f227eabd5ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Show Perplexity and Coherence graph\n",
    "x = range(start, limit, step)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Num Topics')\n",
    "ax1.set_ylabel('Perplexity Score', color=color)\n",
    "ax1.plot(x, perplexity_values, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx() #instantiate a second axes that share the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Coherence Score', color=color)\n",
    "ax2.plot(x, coherence_values, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout() # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf54da",
   "metadata": {
    "id": "16bf54da"
   },
   "source": [
    "If we prefer, we can view the scores in a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18271870",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "18271870",
    "outputId": "8c453587-1f65-4fa4-88a5-c7443dd7b378",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "topics_df =pd.DataFrame({\"Topics Num\": topics_num,\"Coherence Value\": coherence_values,\"Perplexity Value\": perplexity_values})\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80dea58-8017-47fc-b8cd-4d10f3d2396f",
   "metadata": {
    "id": "b80dea58-8017-47fc-b8cd-4d10f3d2396f"
   },
   "source": [
    "## Visualizing Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f644ce4-52f2-4ad3-8838-e9630774ab34",
   "metadata": {
    "id": "7f644ce4-52f2-4ad3-8838-e9630774ab34"
   },
   "source": [
    "We can visualize the LDA results using a nice tool called pyLDAvis.\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "If you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b43888-5b49-4df7-b225-14c0787e18cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "44b43888-5b49-4df7-b225-14c0787e18cf",
    "outputId": "b2aeebdd-23d2-4d33-9e74-95c3a6823bdf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_data = gensimvis.prepare(ldamodel, bows, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f97fa5-1fca-4164-bf64-e37e64c3eda8",
   "metadata": {
    "id": "48f97fa5-1fca-4164-bf64-e37e64c3eda8"
   },
   "source": [
    "Can you guess what each of the identified topic is, based on your 'human' interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeaf83-afbc-4375-ab1a-39d58b5fdb8c",
   "metadata": {
    "id": "69aeaf83-afbc-4375-ab1a-39d58b5fdb8c"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "1. What do you observe about the words of these topics? Are the top words significant to tell what topic it is?  What can you do with these words to improve the model?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "We can see that the words like \"said\", \"mr\", \"year\", \"people\" are appearing across topics as top words. These words are not vey useful to differentiate between the topics. We should remove them. We can do so by treating them as stopwords and update our stopwords list.\n",
    "\n",
    "```python\n",
    "\n",
    "domain_stopwords = [\"said\", \"year\", \"mr\", \"people\"]\n",
    "stop.update(domain_stopwords)\n",
    "\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46eabb-a446-4df5-b75a-d17ae33d093b",
   "metadata": {
    "id": "bc46eabb-a446-4df5-b75a-d17ae33d093b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc602e7e44b83c3583925fd77ac766cf87c9915ce53c5dc0c705dc2e8f6b010c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

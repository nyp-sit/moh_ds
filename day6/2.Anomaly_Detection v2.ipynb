{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-H__blkHgR3"
   },
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zb7aZwNHgR4"
   },
   "source": [
    "We first generate some sample data in two clusters (each one containing n_samples) by randomly sampling the standard normal distribution as returned by numpy.random.randn. One of them is spherical and the other one is slightly deformed.\n",
    "\n",
    "For consistency with the IsolationForest notation, the inliers (i.e. the gaussian clusters) are assigned a ground truth label 1 whereas the outliers (created with numpy.random.uniform) are assigned the label -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoMXgtGZHgR7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples, n_outliers = 120, 40\n",
    "rng = np.random.RandomState(0)\n",
    "covariance = np.array([[0.5, -0.1], [0.7, 0.4]])\n",
    "cluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\n",
    "cluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\n",
    "outliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n",
    "\n",
    "X = np.concatenate([cluster_1, cluster_2, outliers])\n",
    "y = np.concatenate(\n",
    "    [np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2jLUeCDHgR8"
   },
   "source": [
    "We can visualize the resulting clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "kMFpJaSOHgR8",
    "outputId": "4e8504b2-ed59-4070-a645-447de32e241a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.axis(\"square\")\n",
    "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "plt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reksA0ULHgR9"
   },
   "source": [
    "Next, we train the Isolation Forest with the default 100 estimators, default contamination and sub-sampling rate of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "2YXAp3_RHgR9",
    "outputId": "1d24fc02-18bd-4328-fa60-6a75f04170fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#clf = IsolationForest(n_estimators=100, max_samples=100, contamination='auto', random_state=0)\n",
    "clf = IsolationForest(n_estimators=1000, max_samples=100, contamination=0.2, random_state=0)\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAcfDJTKHgR-"
   },
   "source": [
    "We use the class DecisionBoundaryDisplay to visualize a discrete decision boundary. The background color represents whether a sample in that given area is predicted to be an outlier or not. The scatter plot displays the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "CzXXmzZZHgR-",
    "outputId": "31273e58-35b4-4612-ca83-2dc9addee347",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    response_method=\"predict\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "disp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\n",
    "plt.axis(\"square\")\n",
    "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P7-_d4eHgR_"
   },
   "source": [
    "By setting the response_method=\"decision_function\", the background of the DecisionBoundaryDisplay represents the score given by the path length averaged over a forest of random trees.\n",
    "\n",
    "When a forest of random trees collectively produce short path lengths for isolating some particular samples, they are highly likely to be anomalies and the measure of normality is close to 0. Similarly, large paths correspond to values close to 1 and are more likely to be inliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "GUvt6oPaHgSA",
    "outputId": "80733a81-7d97-4870-b2d3-9a1788fbe029",
    "tags": []
   },
   "outputs": [],
   "source": [
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    response_method=\"decision_function\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "disp.ax_.set_title(\"Path length decision boundary \\nof IsolationForest\")\n",
    "plt.axis(\"square\")\n",
    "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "plt.colorbar(disp.ax_.collections[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxt7qvITHgSA"
   },
   "source": [
    "## Exercise\n",
    "Change the hyperparameters of the isolation forest and observe what happens to the decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9-PsWMaHgSB"
   },
   "source": [
    "## Isolation Forest for Credit Card Fraud Detection\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation, as well as the time of transaction and the amount transacted.\n",
    "\n",
    "We first import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2z6T7fhNHgSB",
    "outputId": "508d38fd-5588-4e92-e47f-59960d38f02a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the file creditcard.csv. Write code to read the file and display the first 5 rows\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "    \n",
    "    \n",
    "```\n",
    "df=pd.read_csv('datasets/creditcard.csv')\n",
    "df.head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Enter code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duI2zVs8HgSB"
   },
   "source": [
    "Run the code below to examine the data distribution. Most of the data are valid transactions, with only a small percentage of fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "Z__B3_fRHgSC",
    "outputId": "2ce6272b-35c7-4420-8c42-d3889bcbb27f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.Class.value_counts())\n",
    "df.Class.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL2auOMAHgSC"
   },
   "source": [
    "As a first step, we can train the Isolation Forest using only the non-fraudulent class. For validation, we will use data from both fraudulent and non-fraudulent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OedvwVfmHgSC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "from sklearn.model_selection import train_test_split\n",
    "def get_data(df, clean_train=True):\n",
    "    \"\"\"\n",
    "        clean_train=True returns a train sample that only contains clean samples.\n",
    "        Otherwise, it will return a subset of each class in train and test (10% outlier)\n",
    "    \"\"\"\n",
    "    clean = df[df.Class == 0].copy().reset_index(drop=True)\n",
    "    fraud = df[df.Class == 1].copy().reset_index(drop=True)\n",
    "    print(f'Clean Samples: {len(clean)}, Fraud Samples: {len(fraud)}')\n",
    "\n",
    "    if clean_train:\n",
    "        train, test_clean = train_test_split(clean, test_size=len(fraud), random_state=seed)\n",
    "        print(f'Train Samples: {len(train)}')\n",
    "\n",
    "        test = pd.concat([test_clean, fraud]).reset_index(drop=True)\n",
    "\n",
    "        print(f'Test Samples: {len(test)}')\n",
    "\n",
    "        # shuffle the test data\n",
    "        test.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        \n",
    "        train_X, train_y = train.loc[:, ~train.columns.isin(['Class'])], train.loc[:, train.columns.isin(['Class'])]\n",
    "        test_X, test_y = test.loc[:, ~test.columns.isin(['Class'])], test.loc[:, test.columns.isin(['Class'])]\n",
    "    else:\n",
    "        clean_train, clean_test = train_test_split(clean, test_size=int(len(fraud)+(len(fraud)*0.9)), random_state=seed)\n",
    "        fraud_train, fraud_test = train_test_split(fraud, test_size=int(len(fraud)*0.1), random_state=seed)\n",
    "        print(len(clean_train))\n",
    "        print(len(fraud_train))\n",
    "        \n",
    "        train_samples = pd.concat([clean_train, fraud_train]).reset_index(drop=True)\n",
    "        test_samples = pd.concat([clean_test, fraud_test]).reset_index(drop=True)\n",
    "        \n",
    "        # shuffle\n",
    "        train_samples.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        \n",
    "        print(f'Train Samples: {len(train_samples)}')\n",
    "        test_samples.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        \n",
    "        print(f'Test Samples: {len(test_samples)}')\n",
    "        train_X, train_y = train_samples.loc[:, ~train_samples.columns.isin(['Class'])], train_samples.loc[:, train_samples.columns.isin(['Class'])]\n",
    "        test_X, test_y = test_samples.loc[:, ~test_samples.columns.isin(['Class'])], test_samples.loc[:, test_samples.columns.isin(['Class'])]\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "hW0I14AeHgSD",
    "outputId": "72e19648-76fa-4aa8-d71e-94e080c414d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_data(df)\n",
    "\n",
    "model = IsolationForest(random_state=seed)\n",
    "model.fit(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKhT7iiEHgSE"
   },
   "source": [
    "We can now make the predictions and print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW_y7lZCHgSE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def predict(X):\n",
    "    test_yhat = model.predict(X)\n",
    "    # values are -1 and 1 (-1 for outliers and 1 for inliers), thus we will map it to 0 (inlier) and 1 (outlier) as this is our target variable\n",
    "    test_yhat = np.array([1 if y == -1 else 0 for y in test_yhat])\n",
    "    return test_yhat\n",
    "\n",
    "test_yhat = predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkOUXMH-HgSF",
    "outputId": "dce6bc97-84cf-4127-f398-fa18e5086239",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_classification_report(test_y, test_yhat):\n",
    "    labels = ['Legitimate','Fraudulent']\n",
    "    print(classification_report(test_y, test_yhat, target_names=labels))\n",
    "    \n",
    "get_classification_report(test_y, test_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnM28D0QHgSF"
   },
   "source": [
    "As seen in the classification report, the f1-score for fraudulent class is 0.89, which is quite good. Let us see what happens if our training data had both fraudulent and non-fraudulent data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to perform isolation forest on training data that contain both fraudulent and non-fraudulent data\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "    \n",
    "    \n",
    "```\n",
    "train_X, train_y, test_X, test_y = get_data(df, clean_train=False)\n",
    "model = IsolationForest(random_state=seed)\n",
    "model.fit(train_X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to obtain the classification report for the model above\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "     \n",
    "```\n",
    "test_yhat = predict(test_X)\n",
    "get_classification_report(test_y, test_yhat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkvJMShjHgSG",
    "outputId": "b86f03a5-b12f-4ed5-fb88-1b1799088829"
   },
   "outputs": [],
   "source": [
    "#insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDMmgW4CHgSG"
   },
   "source": [
    "The f1-score for fraudulent cases has dropped to 0.67 although the f1-score for legitimate cases have increased. This indicates that the inclusion of the outliers in the training set has caused the threshold to better identify legitimate points, at the expense of the fraudulent cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Payment Fraud Detection with some Feature Engineering and Explainability\n",
    "\n",
    "Let us now try a synthetic dataset and add some feature engineering and explainability to the anomaly detection model. The dataset here was generated by a program called PaySim which simulates mobile payments based on “aggregated transactional data” from a real company. Given its confidential nature, it is difficult to obtain publicly available transactional data. The dataset comprises a total of 6,362,620 transactions which occurred over a simulated time span of 30 days.\n",
    "\n",
    "The data is stored in the file financial.csv. Based on the code above, read in the file into a pandas dataframe and display the first five rows.\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "df=pd.read_csv('datasets/financial.csv')\n",
    "df.head(5)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is both numerical data, such as the ‘amount’ and ‘oldbalanceOrg’ fields, as well as categorical data, such as the ‘type’ and ‘nameOrig’ fields. These records contain information about the original and the new account balance of each of the two parties involved in the transaction (origin and destination), as well as a separate record of the exact amount (supposed to be) transferred. The ‘step’ field denotes the number of hours passed since the start of the simulation. The ‘isFraud’ column tells us which transactions are indeed fraudulent, whereas the ‘isFlaggedFraud’ column is a simple indicator variable for whether the amount transferred in a given transaction exceeds the threshold of 200,000. This latter field represents the rule based strategy mentioned in the lecture.\n",
    "\n",
    "\n",
    "All numerical features can easily be used as inputs to the model, so the fields ‘amount’, ‘oldbalanceOrg’, ‘newbalanceOrig’, ‘oldbalanceDest’ and ‘newbalanceDest’ will be used as features as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index=df.index)\n",
    "numerical_columns = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "features[numerical_columns] = df[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ‘amount’ field seems to sometimes deviate from the difference between the original and the new balances of one or both of the transaction parties, we will include these differences in the data as two additional features: ‘changebalanceOrig’ and ‘changebalanceDest’.\n",
    "\n",
    "Try to write some code to create the above data and store it in the 'changebalanceOrig' and 'changebalanceDest' columns of the features dataset\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "     \n",
    "```\n",
    "\n",
    "features['changebalanceOrig'] = features['newbalanceOrig'] - features['oldbalanceOrg']\n",
    "features['changebalanceDest'] = features['newbalanceDest'] - features['oldbalanceDest']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ‘step’ field gives us the relative timestamps of all transactions in an hourly resolution, we can derive the (hourly) time of the day when the transaction occurred. To do this we simply transform the ‘step’ field by applying the modulo of 24. Try to write the code for this and store the data in the 'hour' column of the features dataset.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    Click here to see code\n",
    "</summary>\n",
    "     \n",
    "```\n",
    "features['hour'] = df['step'] % 24\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to make use of the information provided in the ‘type’ column. Since our model will only be able to use numerical data, and since there is no logical ordering of the values that the ‘type’ field can assume, we will proceed by one-hot encoding the field into 5 columns, one for each of the possible values of ‘type’. The binary values in the columns indicate whether the content of ‘type’ is equal to the column’s corresponding value. The matrix of one-hot encodings is then appended to our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type_one_hot = pd.get_dummies(df['type'])\n",
    "features = pd.concat([features, type_one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to use Isolation Forest for anomaly detection. To function as a fraud detection system that is as general as possible, we want the following properties in our model:\n",
    "\n",
    "- Makes no assumptions about what an anomaly looks like.\n",
    "- Does not require any flagged data (labels).\n",
    "- Provides a continuous anomaly score, such that the number of identified anomalies can be adjusted depending on the desired strictness.\n",
    "\n",
    "Isolation forest fulfills all of the above requirements and relies on two simple assumptions: Anomalies are few, and anomalies are different.\n",
    "\n",
    "Try to use the code in the previous section to train the Isolation Forest\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import IsolationForest\n",
    "forest = IsolationForest(random_state=0)\n",
    "forest.fit(features)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a continous anomaly score for each data point rather than a binary anomaly indicator which would be dependent on an arbitrarily-chosen threshold value, we call the score samples method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = forest.score_samples(features)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot anomaly score distribution\n",
    "plt.hist(scores, bins=50)\n",
    "plt.ylabel('Number of transactions', fontsize=15)\n",
    "plt.xlabel('Anomaly score', fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output of the Isolation Forest is not a split of the dataset into anomalies and non-anomalies, but rather a list of continuous anomaly scores, one for every entry. This means that, depending on how many anomalies we want to detect (how wide we want to cast our net), we can set a different threshold which determines the data points that are considered as anomalies (i.e. data points with scores below the threshold). A lower anomaly score means that there is a higher chance that the data point is an anomaly.\n",
    "\n",
    "Run the code below to examine the top 5 outlier points identified by the isolation forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_n_outliers = 5\n",
    "top_n_outlier_indices = np.argpartition(scores, top_n_outliers)[:top_n_outliers].tolist()\n",
    "top_outlier_features = features.iloc[top_n_outlier_indices, :]\n",
    "top_outlier_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#alternatively\n",
    "index2=np.argsort(scores)[:5]\n",
    "top_outlier_features = features.iloc[index2, :]\n",
    "top_outlier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to evaluate the result of our model without choosing one particular threshold is by computing the area under the ROC curve of the model output.\n",
    "\n",
    "To have a baseline to compare the isolation forest to, we will use a naive method for anomaly detection in this dataset which consists of treating the money amount transfered as the anomaly score, where higher amounts represent a higher chance of being an anomaly. We will take this approach to compute a naive ROC area under the curve.\n",
    "\n",
    "Finally, we will also add the AUC score that would be obtained by random guessing.\n",
    "\n",
    "The predicted anomalies are evaluated against the 'isFraud' columns which represents the ground truth value of whether the given entry constitutes an anomaly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "# evaluate isolation forest anomaly scores\n",
    "fpr_iforest, tpr_iforest, thresholds_iforest = roc_curve(df['isFraud'], -scores)\n",
    "auc_score_iforest = roc_auc_score(df['isFraud'], -scores)\n",
    "\n",
    "# evaluate naive (amount) anomaly scores\n",
    "fpr_naive, tpr_naive, thresholds_naive = roc_curve(df['isFraud'], df['amount'])\n",
    "auc_score_naive = roc_auc_score(df['isFraud'], df['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, name, auc_score):\n",
    "    plt.plot(fpr, tpr, label=name + ', AUC={}'.format(round(auc_score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr_iforest, tpr_iforest, 'Isolation Forest', auc_score_iforest)\n",
    "plot_roc_curve(fpr_naive, tpr_naive, 'Naive', auc_score_naive)\n",
    "plot_roc_curve([0, 1], [0, 1], 'Random guessing', 0.5)\n",
    "plt.xlabel('False positive rate', fontsize=15)\n",
    "plt.ylabel('True positive rate (recall)', fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(prop={'size': 12})\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the results above tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to understand how the model generates the output. The explanation model we are going to use is called SHAP. For our purposes, what we need to understand about SHAP is that the explanation values it provides tell us about the effect that the value of a feature of a particular data point had on its associated anomaly score. In other words, if we look at a particular output of our model, SHAP values tell us how much each feature of the input contributed to that score, and in which direction (i.e. whether the feature contributed to a higher or a lower anomaly score).\n",
    "\n",
    "First, we need to instantiate an appropriate  Explainer  model. Since we are using a tree-based model it makes sense to use SHAP’s  TreeExplainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install shap\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the SHAP values for a set of 5000 randomly chosen data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(len(features), 5000)\n",
    "shap_values_random = explainer.shap_values(features.iloc[random_indices, :])\n",
    "random_features = features.iloc[random_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the explanation values of a single point, we can use the force_plot function. Let’s display the explanation for the first entry in the randomly chosen dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "shap.initjs\n",
    "dis=shap.force_plot(explainer.expected_value, shap_values_random[0, :], random_features.iloc[0, :],matplotlib=False)\n",
    "shap_html = f\"{shap.getjs()}{dis.html()}\"\n",
    "\n",
    "with open(\"orig_shap.html\", \"w\", encoding='utf8') as file:\n",
    "    file.write(shap_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ‘base value’ in the above plot corresponds to the average output of the model over the training set, whereas f(x) value in bold indicates the model output for this specific datapoint. The purpose of this plot is to show how the individual features of this data point contributed to shifting the model output from its expected (base) value, to the actual value. The values of these individual contributions are SHAP values. Blue bars represent SHAP values that are negative and contributed to a lower anomaly score (making it more likely for that data point to be an anomaly), whereas red bars represent positive SHAP values making the output higher (i.e. suggesting that this data point is normal). The sum of all SHAP values is equal to the difference between base value and model output value. Note that the SHAP explainer works with raw anomaly scores, whereas on the histogram earlier we were looking at normalized values. The meaning stays the same: a lower score implies higher chance of being an anomaly.\n",
    "\n",
    "In the above example we can see that, while a couple of features were suggestive of this data point being anomalous, such as the amount field (presumably because it is a rather high amount), most of the features indicate that this data point is rather normal.\n",
    "\n",
    "One issue with the previous plot is the fact that the different values of the ‘type’ field are treated as separate features with binary values due to the one-hot-encoding we applied in the beginning. To solve this, we can simply undo the one-hot-encoding in the feature matrix and add all of the corresponding SHAP values to a single value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def condense(features, shap_values, indices):\n",
    "    features_condensed = features.drop(columns=['CASH_IN', 'PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT'])\n",
    "    features_condensed['type'] = df['type']\n",
    "    selected_features_condensed = features_condensed.iloc[indices, :]\n",
    "    \n",
    "    shap_values_without_type = shap_values[:, :-5]\n",
    "    shap_values_type_sum = shap_values[:, -5:].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    shap_values_condensed = np.concatenate([shap_values_without_type, shap_values_type_sum], axis=1)\n",
    "\n",
    "    return selected_features_condensed, shap_values_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_features_condensed, shap_values_random_condensed = condense(features, shap_values_random, random_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code above to generate a new Shap plot usign these condensed values and save it to 'condensed_shap.html'. \n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "from IPython.display import display, HTML\n",
    "shap.initjs\n",
    "dis=shap.force_plot(explainer.expected_value, shap_values_random_condensed[0, :], random_features_condensed.iloc[0, :],matplotlib=False)\n",
    "shap_html = f\"{shap.getjs()}{dis.html()}\"\n",
    "#display(HTML(shap_html))\n",
    "with open(\"condensed_shap.html\", \"w\", encoding='utf8') as file:\n",
    "    file.write(shap_html)\n",
    "#display(HTML(shap_html))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though SHAP values are local explanations, i.e. they explain the contributions of features on single data points, we can gain more general insights about the decision our model makes by aggregating many of these local explanations to discover global trends. Let us first take a look at a summary of the SHAP value for the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_random_condensed, random_features_condensed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful tool that the SHAP library provides to gain such insights is the dependence_plot function. It shows us how, across many data points, a SHAP value of a specific feature (y-axis) depends on the feature’s value (x-axis). Dots in the plot represent individual data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    " 'changebalanceDest',\n",
    " shap_values_random,\n",
    " random_features,\n",
    " interaction_index=None,\n",
    " xmax='percentile(99)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the SHAP value of ‘changebalanceDest’ is small for values close to zero, suggesting small changes in the receiver account’s balance are common. The more the absolute value increases, the more the SHAP value contributes to making the anomaly score lower (i.e. increasing the chance of being an anomaly). We can see that the most significant SHAP values are in the realm of very large positive numbers.\n",
    "\n",
    "The dependence_plot also allows us to take into consideration the effect of another feature with the interaction_index argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    " 'changebalanceDest',\n",
    " shap_values_random,\n",
    " random_features,\n",
    " interaction_index='CASH_OUT',\n",
    " xmax='percentile(99)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that, whether the ‘type’ field of a data point is equal to CASH_OUT or not also correlates with how the SHAP value for ‘changebalanceDest’ behaves. It seems as though large sums of money become indicators for an anomaly more quickly if the ‘type’ field is equal to CASH_OUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    " 'hour',\n",
    " shap_values_random,\n",
    " random_features,\n",
    " interaction_index='PAYMENT',\n",
    " xmax='percentile(99)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also detect an interesting interaction between the SHAP value of the ‘hour’ feature and whether the ‘type’ feature is equal to PAYMENT.\n",
    "While a ‘hour’ value between 0 and 5 is generally indicative of being an anomaly, this effect is more pronounced for transactions of the PAYMENT type. Interestingly, this effect is not there or even reversed during other times of the day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxPBoI-jHgSH"
   },
   "source": [
    "## DBSCAN\n",
    "\n",
    "Let us now use a different method, DBSCAN for anomaly detection. Recall that DBSCAN is a density based clustering approach that clusters data points based on continuous regions of high point density and determines the ideal number of clusters to be formed. In contrast to k-means, not all points are assigned to a cluster, and we are not required to declare the number of clusters (k). However, the two key parameters in DBSCAN are min_samples (to set the minimum number of data points required to determine a core point) and eps (max allowed distance between two points to put them in the same cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jIBm6qB1Ukb"
   },
   "source": [
    "Let's first try a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "W757pDSE1XZe",
    "outputId": "3c87ee73-ad67-47a4-bdd1-8b8b70f219a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X_train = np.array([[60,36], [100,36], [100,70], [60,70],\n",
    "    [140,55], [135,90], [180,65], [240,40],\n",
    "    [160,140], [190,140], [220,130], [280,150], \n",
    "    [200,170], [185, 170]])\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrhjQSqK5YNo"
   },
   "source": [
    "We can use DBSCAN with eps=45 and min_samples=4 to perform anomaly detection. There are 6 core points found by the algorithm, 2 clusters and a couple of outliers (noise points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPUxS9OY1_6Z",
    "outputId": "f9ee7e76-11eb-4af3-c689-19bc1afde866",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 45\n",
    "min_samples = 4\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_train)\n",
    "labels = db.labels_\n",
    "print(labels)\n",
    "\n",
    "print(db.core_sample_indices_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22kE-fW-5XHU"
   },
   "source": [
    "We can visualize the clusters as below. Points in cluster 0 are colored red, points in cluster 1 are colored green, outlier points are colored black and core points are marked with '*'s. Two points are connected by an edge if they are within the epsilon neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "uotR8ccN2OOB",
    "outputId": "54080003-3b01-4567-e195-05cb9de0727e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "colors = ['r', 'g', 'b', 'k']\n",
    "for i in range(len(X_train)):\n",
    "    plt.scatter(X_train[i,0], X_train[i,1], \n",
    "                s=300, color=colors[labels[i]], \n",
    "                marker=('*' if i in db.core_sample_indices_ else 'o'))\n",
    "                                                            \n",
    "    for j in range(i+1, len(X_train)):\n",
    "        if dist(X_train[i], X_train[j])  < eps:\n",
    "            plt.plot([X_train[i,0], X_train[j,0]], [X_train[i,1], X_train[j,1]], '-', color=colors[labels[i]])\n",
    "            \n",
    "plt.title('Clustering with DBSCAN', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkEI-8VynRMA"
   },
   "source": [
    "## DBSCAN on credit card fraud dataset\n",
    "\n",
    "We first scale and normalize the train_X data from above. Recall from the previous practical how to do this.\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "scaler=StandardScaler().fit(train_X)\n",
    "X_s = scaler.transform(train_X)\n",
    "X_norm = pd.DataFrame(normalize(X_s))\n",
    "X_norm.head()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tku_Tj7CWF5p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SV2yCu5n_8p"
   },
   "source": [
    "We will fit the DBSCAN model using eps 0.65 and min_samples as 5. Obtain the class that each sample was assigned to and store it in the variable \"labels\". Recall that anomalies have ‘-1’.\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "db_model = DBSCAN (eps=0.65, min_samples=5).fit(X_norm)\n",
    "labels=db_model.labels_\n",
    "np.unique(labels)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSAkjtMRoGCy",
    "outputId": "7eae86c7-8e3e-4738-a1b7-34cbbb61e850"
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-X68X6zvGaP"
   },
   "source": [
    "We can visualize a logarithmic histogram of the labels, and count the number of outliers identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "A5ENikULeBT1",
    "outputId": "33d724f8-9e80-452e-92dd-13b232f28534",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(labels, bins=len(np.unique(labels)),log=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwoIfSarertN",
    "outputId": "802af17e-4cb4-45c5-cf1a-f92256991ad9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters=len(np.unique(labels))-1\n",
    "anomaly=list(labels).count(-1)\n",
    "print(f'Clusters: {n_clusters}')\n",
    "print(f'Abnormal points: {anomaly}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pW_SfEZnNwy"
   },
   "source": [
    "Let us check the accuracy of DBSCAN on the training set. Try to write some code to do this.\n",
    "\n",
    "<details><summary>Click here for answer</summary> \n",
    "<br/>\n",
    "\n",
    "```\n",
    "\n",
    "labels2=np.array([1 if y == -1 else 0 for y in labels])\n",
    "def get_classification_report(test_y, test_yhat):\n",
    "    label_name = ['Legitimate','Fraudulent']\n",
    "    print(classification_report(test_y, test_yhat, target_names=label_name))\n",
    "get_classification_report(train_y, labels2)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLbVkhswxfe9",
    "outputId": "be8fa2a0-58a8-4063-f1de-1b6b718772b9"
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztglDKJoVnMg"
   },
   "source": [
    "The accuracy of DBSCAN doesn't seem good with the current parameters. Isolation Forest has a better performance. The anomalies detected by DBSCAN from this dataset are not the actual anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dCFQ7rGgyhW"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Try adjusting some of the parameters of DBSCAN and observe what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (Optional)\n",
    "\n",
    "Try using DBSCAN on the mobile payment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBhTpJ3-HgSH"
   },
   "source": [
    "## Additional Resource (Optional): Pycaret\n",
    "Pycaret is an Automated Machine Learning (AutoML) tool that can be used for both supervised and unsupervised learning. It contains many anomaly detection models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLI6vJVPHgSH"
   },
   "source": [
    "We will be performing anomaly detection on the Wisconsin Breast Cancer (Diagnostic) dataset from UCI Machine Learning Repository which contains features computed digitized image of a fine needle aspirate of a breast mass and the diagnosis if the mass is benign (B) or malignant (M). This dataset commonly used for demonstrating supervised machine learning where a model is trained to predict the diagnosis. For the purpose of demonstrating unsupervised anomaly detection, we will ignore the diagnosis. We first split the data into the training set and reserve a small “unseen” set for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "RYww24VEHgSR",
    "outputId": "450cfcec-c76b-4f3f-cfd2-c6664eeaaf74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycaret.anomaly import *\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "df = load_breast_cancer(as_frame=True)['data']\n",
    "df_train = df.iloc[:-10]\n",
    "df_unseen = df.tail(10)\n",
    "\n",
    "df_unseen.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNAueBN8HgSR"
   },
   "source": [
    "Next, we will setup Pycaret to use the dataset. To use Pycaret, we will need to first call the setup function as below. Setting the silent parameter to True automatically confirms the input of data types when setup is executed. If silent is set to False, Pycaret requires the user to do manual confirmation of the input data types as shown in the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p80IhOCBHgSS",
    "outputId": "e580f943-fca9-4fdc-bfd8-5277ed0aae12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "anom = setup(data = df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAf6W9nOHmoO"
   },
   "source": [
    "We can check the anomaly detection models available in Pycaret. The reference column indicates which source package the model was built from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "Cz0YeDEqIMZQ",
    "outputId": "886b8a77-c6fa-45aa-eb2f-d4e7e100f08a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OleAgD6bIg6T"
   },
   "source": [
    "Next, we will train an anomaly detection model. Let's load the iforest model that we have seen previously, with fraction parameter = 0.05. The fraction parameter is the contamination parameter that we have seen previously and indicates the amount of outliers present in the dataset. It has a default value of 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418,
     "referenced_widgets": [
      "2345726f86504dc3b675d88f815c2cea",
      "5b9000f1712a423ab90e4b42a3d4dc06",
      "f1d02c2d10e642a8921a4e01537de5e7"
     ]
    },
    "id": "bTbfQGqEIOkC",
    "outputId": "6cd9e733-b1fc-4198-edb1-ef5a4b661699",
    "tags": []
   },
   "outputs": [],
   "source": [
    "anom_model = create_model(model = 'iforest', fraction = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGTtUYTHKPMT"
   },
   "source": [
    "We can now train the model using the *assign_model* function. This scores the training dataset using the trained model and returns the prediction of the model, concatenated with the training data. The Anomaly column is binary where 1 indicates that the record is anomalous and 0 indicates that it is normal. The Anomaly_Score column gives the raw score for the record, where negative indicates that the record is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJJi70ojKVkt",
    "outputId": "bfba0e75-0876-4494-f2ea-d5c975198ccd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = assign_model(anom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yCX_MVOLiuM",
    "outputId": "e9024b12-511b-489b-a411-860ce2adda96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27I5VlU8PESd"
   },
   "source": [
    "We can visualize the high dimensional results in lower dimensions using data visualization non-linear graph based methods such as t-SNE or UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "1d5b4613153142f68aba726685eef5b2",
      "a6f525c2a0574b5884a7fa873e1be069",
      "1168f77fc80e476a8d2d0e96c0e40342"
     ]
    },
    "id": "8uqkdC4SLsk1",
    "outputId": "7e5060e9-1519-4d55-bb05-cd57d14e85c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(anom_model, plot = 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889,
     "referenced_widgets": [
      "03cb0660aaba4cb280028a2ef9ddd558",
      "9fe7c98aab714c88a40642ba0867ce2d",
      "5d54e3cf4d974cef8c39a714cd67c759"
     ]
    },
    "id": "EoiHLctnMQum",
    "outputId": "2572bc04-3535-4307-f83b-f0444f894648",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(anom_model, plot = 'umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VrLo6JOP3QI"
   },
   "source": [
    "Finally, we can save the model, load the saved model and use it to make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgvffCE7QBZg",
    "outputId": "4bb2d586-198f-4a1a-e1dd-3e87d0f60c86",
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model(model = anom_model, model_name = 'iforest_model')\n",
    "loaded_model = load_model('iforest_model')\n",
    "loaded_model.predict(df_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6-GHWuhSAdl"
   },
   "source": [
    "We can look also look at the probabilities, as well as the anomaly scores using the following functions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_1-vX-eQUHP",
    "outputId": "92ed165d-f2f4-4999-ee36-134dc63d4df5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model.predict_proba(df_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySxxOpQMQfPy",
    "outputId": "4b386f6b-0fed-488d-a3c1-a0cf30fcc527",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model.decision_function(df_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXWl1kRpSX8c"
   },
   "source": [
    "However, the results are not very accurate with the default fraction value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntQZEwxYQj91",
    "outputId": "7f437342-47be-41b4-d22c-f57f06882ed8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2=load_breast_cancer(as_frame=True)['target']\n",
    "df2_unseen = df2.tail(10)\n",
    "print(df2_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXLsBw6TSn5P"
   },
   "source": [
    "## Additional Exercise (Optional)\n",
    "\n",
    "1. Try changing the fraction/contamination value and see if the prediction accuracy increases. Discuss why or why not.\n",
    "2. Try using the pycaret package on the credit card fraud or the mobile payment dataset and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gxt7qvITHgSA",
    "_dCFQ7rGgyhW",
    "xBhTpJ3-HgSH",
    "KXLsBw6TSn5P"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc602e7e44b83c3583925fd77ac766cf87c9915ce53c5dc0c705dc2e8f6b010c"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03cb0660aaba4cb280028a2ef9ddd558": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Processing: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fe7c98aab714c88a40642ba0867ce2d",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d54e3cf4d974cef8c39a714cd67c759",
      "value": 3
     }
    },
    "1168f77fc80e476a8d2d0e96c0e40342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1d5b4613153142f68aba726685eef5b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Processing: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6f525c2a0574b5884a7fa873e1be069",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1168f77fc80e476a8d2d0e96c0e40342",
      "value": 3
     }
    },
    "2345726f86504dc3b675d88f815c2cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Processing: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b9000f1712a423ab90e4b42a3d4dc06",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1d02c2d10e642a8921a4e01537de5e7",
      "value": 3
     }
    },
    "5b9000f1712a423ab90e4b42a3d4dc06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d54e3cf4d974cef8c39a714cd67c759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9fe7c98aab714c88a40642ba0867ce2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6f525c2a0574b5884a7fa873e1be069": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1d02c2d10e642a8921a4e01537de5e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

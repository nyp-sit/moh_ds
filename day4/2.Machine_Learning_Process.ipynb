{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a14028cd-85ae-493d-af64-f35112d0c53d"
   },
   "source": [
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlcN8ctTgVbP"
   },
   "source": [
    "# End-to-end Machine Learning Process\n",
    "\n",
    "In this lab, we will go through end-to-end process of building a regression model to predict housing prices.\n",
    "\n",
    "At the end of the session, you will learn how to:\n",
    "\n",
    "\n",
    "1. Perform exploratary data analysis\n",
    "2. Perform data preparation\n",
    "3. Train and validate model\n",
    "4. Fine Tune Model\n",
    "5. Test the model\n",
    "6. Package the model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUd852Bs29m6"
   },
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5T_7DFP2yxx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNk6srtW3swN"
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "We will be using the California housing Prices dataset.  This dataset was based on data from the 1990 California census. You can see a description of the data here:\n",
    "https://www.kaggle.com/datasets/camnugent/california-housing-prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeQwwacq2nLf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzNmjWJsib_P"
   },
   "source": [
    "## Understanding the data\n",
    "\n",
    "As in all Machine Learning project, it is important to have a good understanding of your data. We will be doing some exploratory data analysis as our next step. But before we delve further into it, let's just take a quick look at our data.  We can first examine some samples, using `Dataframe.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnhdopmXdsOO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKooBXQmdsOO"
   },
   "source": [
    "The `info()` method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWU_gij3yrfJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz7Hbqf9dsOU"
   },
   "source": [
    "**Question**\n",
    "\n",
    "1. How many samples we have?\n",
    "2. Do we have any missing values? Which feature(s) have missing values?\n",
    "3. Which feature(s) is a categorical feature?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "1. 20640 samples in total\n",
    "2. Yes, we have missing values. The samples have 207 missing 'total_bedrooms' values.\n",
    "3. ocean_proximity is a categorical value, which has 'object' as its data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqC59BQQdsOU"
   },
   "source": [
    "The `describe()` method shows a summary of the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qeSLDN5dsOV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNlVhlfrdsOV"
   },
   "source": [
    "Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U01G72EVdsOV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')\n",
    "df.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2rX2YsSdsOV"
   },
   "source": [
    "You noticed that some attributes have a skewed-right distribution, so you may want to transform them (e.g.,\n",
    "by computing their logarithm) when preparing data later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEvKwfsbdsOV"
   },
   "source": [
    "**Question**\n",
    "\n",
    "What do you notice from the histogram plot about median housing value? Will there be potential problem?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "The histogram shows a large count of houses at the maximum price.  This is due to the way the data is collected, for example, housing prices are capped at a maximum value (e.g. 500,001)\n",
    "\n",
    "This may be a serious problem since it is your target attribute (your labels). Your machine learning algorithms may learn that prices never go beyond that limit. If you need to predict values beyond $500,001, then you should collect proper labels for the districts whose labels are capped. Or you can remove those districts from the training and test set.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYnOhQpIAORC",
    "tags": []
   },
   "source": [
    "## Splitting Data into Train and Test Set\n",
    "Before we proceed with more data exploration, it is often a good practice for us to first set aside a part of our dataset as test set, so as to prevent us from snooping information/pattern from the test set and 'overfit' ourselves (and eventually our model) to the test set.\n",
    "\n",
    "We can either random shuffle the data and split them into train/test split using scikit-learn's `train_test_split()` method, e.g.\n",
    "\n",
    "```\n",
    "train_set, test_set = train_test_split(df, 0.2)\n",
    "```\n",
    "\n",
    "This is\n",
    "generally fine if your dataset is large enough (especially relative to the\n",
    "number of attributes), but if it is not, you run the risk of introducing a\n",
    "significant sampling bias.  Your train set may not have a representative distribution as your eventual test set or real-world data.\n",
    "\n",
    "If based on the domain experts inputs, who feel that income distribution is a key for good prediction, we want to make sure our train and test set has the same income distribution. So we may want to split in such a way that train/test set has same distribution of income categories, e.g. This can be done by stratified sampling.\n",
    "\n",
    "Before that let's take a closer look at the income distribution using histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPomfmUbD03_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.hist(df.median_income)\n",
    "sns.histplot(data=df, x='median_income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9xQoAieEXXU"
   },
   "source": [
    "**Creating income categories**\n",
    "\n",
    "\n",
    "Most median income values are clustered around 1.5 to 6 (i.e.,  \\\\$15,000 to \\\\$60,000), but some\n",
    "median incomes go far beyond 6. It is important to have a sufficient number\n",
    "of instances in your dataset for each stratum, or else the estimate of a\n",
    "stratum’s importance may be biased. This means that you should not have too\n",
    "many strata, and each stratum should be large enough.\n",
    "\n",
    "We can use the [`pd.cut()`](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) to bin the median income into 5 categories (e.g. $[0, 1.5], [1.5, 3.0], [3.0, 4.5], [4.5, 6]$ and $[6, \\infty]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcnE2Y5DdsOX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"income_cat\"] = pd.cut(df[\"median_income\"],\n",
    "                          bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                          labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrLj5-33dsOX"
   },
   "source": [
    "Let us find out the number of samples for each categories 1, 2, ... 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OI1_97kdsOX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"income_cat\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5ycAd0kdsOY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "df[\"income_cat\"].value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LcggVHYU3YB"
   },
   "source": [
    "### Using Stratified Sampling to Split data\n",
    "\n",
    "Stratified random sampling is a method of sampling that involves the division of a population into smaller sub-groups known as strata. In stratified random sampling, the strata are formed based on members' shared attributes or characteristics such as income or educational attainment.  The following code shows you how we can use Stratified Sampling to split the data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teg9milD2Qdc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(df, shuffle=True,\n",
    "                                                   stratify=df['income_cat'],\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4Om3oWQdsOY"
   },
   "source": [
    "In the code cell below, we will compute and display the percentage of each income categories for 'Stratified' and 'Random' splitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAmYzNZcrV-z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def income_cat_props(data):\n",
    "    # compute the percentage of data across different categories\n",
    "    return data['income_cat'].value_counts()/len(data)\n",
    "\n",
    "rand_train_set, rand_test_set = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    'Overall': income_cat_props(df),\n",
    "    'Stratified': income_cat_props(strat_test_set),\n",
    "    'Random': income_cat_props(rand_test_set)\n",
    "}).sort_index()\n",
    "\n",
    "compare_props['Rand. %error'] = 100 * compare_props['Random'] / compare_props['Overall'] - 100\n",
    "compare_props['Strat. %error'] = 100 * compare_props['Stratified'] / compare_props['Overall'] - 100\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AazWTvU7UxLw"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv6HI_TL0gjW"
   },
   "source": [
    "We shall further explore out train dataset to gain more insights.   Let's create a copy of the housing data so that we can experiment with it without affecting the training set.  Use the copy method to create a new copy of the stratified training data set we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhLV6K4RdsOZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86ea5QL6XpnE"
   },
   "source": [
    "### Visualize geographical data\n",
    "\n",
    "Because the dataset includes geographical information (latitude and longitude), it is a good idea to create a scatterplot of all the districts to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9a5kwNDuvcp3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter',  x='longitude', y='latitude', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5ALI2ccPFFL"
   },
   "source": [
    "Now, let's get a bit more insight into whether how population, and median house values are related to the location. We can use the size of marker to represent the population variable, and color to represent population variable. We choose a predefined colormap 'jet' which ranges from blue (low value) to red (high value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "w-HA5Zd40cJt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
    "             s=housing['population']/100, label='population',\n",
    "             c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3crmpmULdsOa"
   },
   "source": [
    "**Question**\n",
    "\n",
    "What can you conclude from this scatterplot?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "This plot tells you that the housing prices are very much related to the\n",
    "location (e.g., close to the ocean) and to the population density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slBdVYQRY8CZ",
    "tags": []
   },
   "source": [
    "### Looking for Correlations\n",
    "\n",
    "We can compute the standard correlation coefficient (Pearson's r) between every pair of attributes using the corr() method, and examine how much each attribute correlate with the median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bBECn0Eb2Mpn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV1dBbQFdsOa"
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "Which variable(s) have high positive correlation with median_housing_value?\n",
    "\n",
    "<details><summary>Click Here for Answer</summary>\n",
    "\n",
    "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation. In our case, median house value and median income have strong positive correlation: when median median goes up, median house value goes up as well. When the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to 0 mean that there is no linear correlation.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDmlOm_ZZhoN"
   },
   "source": [
    "Another way to check for correlation between attributes is to use `sns.pairplot` function to plot every numerical attribute against every other numerical attribute.  The most promising attribute the predict the median house value seems to be the median income.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "aMcC1Yhd261q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.pairplot(housing[attributes])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjWe62VAb6vG"
   },
   "source": [
    "Looking at the correlation scatterplots, it seems like the most promising attribute to predict the median house value is the median income, so we zoom in on their scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL6ivLnQY5v8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=housing, x='median_income', y='median_house_value', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoJle0FIdsOb"
   },
   "source": [
    "#### Question 2\n",
    "\n",
    "Do you notice something peculiar about the scatter plot?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "The price cap we noticed earlier is clearly visible as a horizontal line at \\\\$500,000. But the plot also reveals other less obvious straight lines: a horizontal line around \\\\$450,000, another around \\\\$350,000.\n",
    "\n",
    "You may want to try removing the corresponding districts to prevent your algorithms from learning this data quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UU3G2cB7xFD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strat_train_set = strat_train_set.loc[(strat_train_set.median_house_value != 500001.0) & (strat_train_set.median_house_value != 350000.0)]\n",
    "# strat_test_set = strat_test_set.loc[(strat_test_set.median_house_value != 500001.0) & (strat_test_set.median_house_value != 350000.0)]\n",
    "# housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Lw6jHtJduUL"
   },
   "source": [
    "### Experimenting with Attribute Combinations\n",
    "\n",
    "One last thing you may want to do before preparing the data for machine learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. You create these new attributes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WtW3XHnP7u0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing['rooms_per_household'] = housing['total_rooms']/housing['households']\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population']/housing['households']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy26RkbQdsOb"
   },
   "source": [
    "Now we look at the correlation matrix again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqGQ2GWXdsOb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQaZAG-VdsOb"
   },
   "source": [
    "It looks like the new bedrooms_ratio attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnTMPaRg_9Vh"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "After gaining some understanding of our data, we are ready to prepare our data for machine learning. We will revert to our clean dataset, and separate our features (predictors) and labels (target values), i.e. the median house value.\n",
    "\n",
    "### Separate features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqhKVK1RZTa-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate the target values from predictors\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels=strat_train_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf_Z7ol9dsOc"
   },
   "source": [
    "### Clean the data\n",
    "\n",
    "We observed earlier that *total_bedrooms* feature has some missing values. We can either:\n",
    "1. Get rid of the corresponding rows that has missing values for *total_bedrooms*\n",
    "2. Get rid of the feature totally\n",
    "3. Set the missing values to some value, which can be zero, the mean, the median, etc. This is called imputation.\n",
    "\n",
    "Scikit Learn provides a handy class to fill in the missing values: `Imputer`.\n",
    "\n",
    "Let's us just use the median as replacement values.  As median only make sense for numerical values, we will separate numerical features from categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjskpMFsdsOc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "housing_cat =  housing['ocean_proximity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QIZMkOMdsOc"
   },
   "source": [
    "We will then create an instance of imputer, specifying median as our replacement values, and fit (train) the imputer on our training data to learn the statistics, i.e the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qH4WdxmcdsOc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(housing_num)\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y28PfhazdsOc"
   },
   "source": [
    "Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muN3DGsMdsOc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPNHhH2zdsOd"
   },
   "source": [
    "Notice that after the transformation, the result is no more a dataframe, but numpy array. So let us just convert `X` back to Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpgsSEEQYVs3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = housing_num.index)\n",
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R921C5-GdsOd"
   },
   "source": [
    "### Handling Text and Categorical Data\n",
    "\n",
    "In this dataset, there is just one attribute that is text: the ocean_proximity attribute.  Let's just see what are the different values that this attribute has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55tKSTaYYWnq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyeddNQwdsOd"
   },
   "source": [
    "We can see that there is only a limited number of possible values, which means this is a categorical attribute.  Most Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. One way to do this is to assign a number to each category, e.g. `1<H OCEAN = 0, INLAND = 1, NEAR OCEAN = 2, etc.`. This can be done using `OrdinalEncoder()` in scikit-learn. However, one issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values, which may not be a valid assumption.  A better encoding for categorical data is to use one-hot encoding, using `OneHotEncoder()` class in scikit-learn.\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlKy3RfOdsOd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xha0axhObrq2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(drop=\"first\")\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat.values.reshape(-1, 1))\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o0-MCdn5OvB"
   },
   "source": [
    "We can get the list of categories using the encoder’s categories_ instance variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFcSpFlEjPHP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri6gE1mQdsOe"
   },
   "source": [
    "### Feature Scaling and Transformation\n",
    "\n",
    "#### Scaling\n",
    "\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: *min-max scaling* and *standardization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hq5_c0bdsOe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89WYqenedsOe"
   },
   "source": [
    "When a feature’s distribution has a heavy tail, both min-max scaling and standardization will squash most values into a small range. Machine\n",
    "learning models generally don’t like this at all. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this is to replace the feature with its logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LvxBuWpdsOe"
   },
   "source": [
    "For example, the *population* feature has a long tail. After we apply log transform, it now more closer to a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_XUdbJIdsOe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2)\n",
    "sns.histplot(data=housing_tr['population'], ax=ax[0])\n",
    "sns.histplot(data=housing_tr['population'].apply(np.log), ax=ax[1])\n",
    "ax[0].set_xlabel('population')\n",
    "ax[1].set_xlabel('log of population')\n",
    "ax[0].set_ylabel(\"number of districts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTMANIIQdsOe"
   },
   "source": [
    "#### Custom Transformer\n",
    "\n",
    "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom transformations, cleanup operations, or combining specific attributes.\n",
    "\n",
    "For transformations that don’t require any training, you can just write a function that takes a NumPy array as input and outputs the transformed\n",
    "array. For example, we can implement the log transform in the above cell as a FunctionTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B124w7ftdsOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "log_transformer = FunctionTransformer(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEFJ3hmUdsOf"
   },
   "source": [
    "Custom transformers are also useful to combine features. For example, here’s a FunctionTransformer that computes the ratio between the input features 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAMPm48JdsOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "ratio_transformer.transform(np.array([ [1., 2.], [3., 4.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0wYGOetdsOf"
   },
   "source": [
    "Previously, we showed that some derived features such as bedroom ratio (total_bedrooms/total_rooms) are more informative than total_bedrooms alone. Below we show how we can create a transformer for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_7xXcsLdsOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"] # feature names out\n",
    "\n",
    "ratio_transformer = FunctionTransformer(column_ratio, feature_names_out=ratio_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2cU2-TudsOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_rooms = housing[['total_bedrooms', 'total_rooms']]\n",
    "ratio_transformer.fit_transform(housing_rooms.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uYluDJCdsOf"
   },
   "source": [
    "#### Transformation Pipeline\n",
    "\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides `make_pipeline()` to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then\n",
    "scale the input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox8-ZhWkdsOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "sklearn.set_config(display='diagram')  # display pipeline as diagram\n",
    "\n",
    "## Alternate way of creating a named pipeline\n",
    "# num_pipeline = Pipeline([\n",
    "#     (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"standardize\", StandardScaler()),\n",
    "# ])\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJ_03bXXdsOg"
   },
   "source": [
    "You can now use the pipeline to transform your housing_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC2HlL5OdsOg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_prepared[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOR74bdOD2cH"
   },
   "source": [
    "When you call the pipeline’s `fit()` method, it calls `fit_transform()` sequentially on all the transformers, passing the\n",
    "output of each call as the parameter to the next call until it reaches the final estimator, for which it just calls the `fit()` method.\n",
    "The pipeline exposes the same methods as the final estimator. In this example the last estimator is a `StandardScaler`, which is a transformer, so the pipeline also acts like a transformer. If you call the pipeline’s `transform()` method, it will sequentially apply all the transformations to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIDtVdhddsOg"
   },
   "source": [
    "If you want to recover a nice DataFrame, you can use the pipeline’s `get_feature_names_out()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjl2pq5kdsOg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(housing_num_prepared,\n",
    "                                       columns=num_pipeline.get_feature_names_out(),\n",
    "                                       index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msg1N6wydsOg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing_num_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ixBz909dsOg"
   },
   "source": [
    "Similarly we can define a pipeline for categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZPy6UULdsOg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))\n",
    "cat_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4B23fcodsOh"
   },
   "source": [
    "#### Column Transformer\n",
    "\n",
    "So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single\n",
    "transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a ColumnTransformer. For example, the following ColumnTransformer will apply `num_pipeline` to numerical attributes and `cat_pipeline` to categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjfGlEZskzYU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL6AjKqNdsOh"
   },
   "source": [
    "If we don't care about naming the individual transformer (pipeline), we can use `make_column_transformer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WWgYbYAdsOh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "                    (num_pipeline, num_attribs),\n",
    "                    (cat_pipeline, cat_attribs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynRp3QHodsOh"
   },
   "source": [
    "Scikit-Learn provides a `make_column_selector()` function that returns a selector function you can use to automatically select all the features of a given type, such as numerical or categorical. You can pass this selector function to the ColumnTransformer instead of column names or indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVp9BqRadsOh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoyX_oK0dsOh"
   },
   "source": [
    "#### Integrating all the different transform pipeline\n",
    "\n",
    "Now let us apply all the different transformations we have experimented with earlier, and put them into a single ColumnTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrzwEqhXdsOh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"] # feature names out\n",
    "\n",
    "ratio_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "    StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "\n",
    "default_num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline, [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline, [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline, [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"population\", \"households\", \"median_income\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),],\n",
    "    remainder=default_num_pipeline)\n",
    "\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8wqOQbkdsOi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4Rok44_dsOi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWOF15aOdsOi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing_prepared = pd.DataFrame(housing_prepared,\n",
    "                                   columns=preprocessing.get_feature_names_out(),\n",
    "                                   index=housing.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vH2vUCOrdsOi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qz0qSxvFa1G"
   },
   "source": [
    "## Select and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSE5BDyrFeLl"
   },
   "source": [
    "We are now ready to select and train a machine learning model. Let's train a very basic linear regression model to get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zflGnMOLbsMm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
    "lin_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBSW0oTfGKHk"
   },
   "source": [
    "Try it out on a few instances from the training set.  It works, but the predictions are not great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UryoffgpdsOj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_predictions = lin_reg.predict(housing)\n",
    "print(\"predicted:\", housing_predictions[:5].round(-2)) # -2 = rounded to the nearest hundred\n",
    "\n",
    "print(\"actual:\", housing_labels.iloc[:5].values)\n",
    "\n",
    "print(\"diff:\", housing_predictions[:5] - housing_labels.iloc[:5].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8zv8V11Gell"
   },
   "source": [
    "Let us measure the RMSE of this regression model RMSE, using scikit-learn `mean_squared_error()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2xYGQh9qKer",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
    "print(lin_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTcplotsdsOj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# housing_labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIYvYwpTGwfM"
   },
   "source": [
    "Clearly not a great score: the median_housing_values of most districts range between \\\\$120,000 and \\\\$265,000, so a typical prediction error of \\\\$70,495 is really not very satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMRxWQjdsOj"
   },
   "source": [
    "### Evaluation using Cross Validation\n",
    "\n",
    "Note that so far we are only evaluating the model on our training set. How do we know the performance on the test set (unseen data). One way is to a validation set. We can use the `train_test_split()` function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set.\n",
    "\n",
    "A great alternative is to use Scikit-Learn’s *k-fold* cross-validation feature. The following code randomly splits the training set into 3\n",
    "nonoverlapping subsets called folds, then it trains and evaluates the decision tree model 5 times, picking a different fold for evaluation every time and using the other 4 folds for training. The result is an array containing the 5 evaluation scores.\n",
    "\n",
    "*Note*: A better choice of number of folds is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y10lIRQbdsOj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
    "linreg_rmses = cross_validate(lin_reg,\n",
    "                              housing,\n",
    "                              housing_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\",\n",
    "                              return_train_score=True,\n",
    "                              cv=5)\n",
    "\n",
    "print(\"rmses (train): \", -linreg_rmses['train_score'])\n",
    "print(\"average train rmse: \", -linreg_rmses['train_score'].mean())\n",
    "print(\"rmses (val):\", -linreg_rmses['test_score'])\n",
    "print(\"average val rmse:\", -linreg_rmses['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cc2I4JEdsOk"
   },
   "source": [
    "We can try Polynomial regression by adding powers to each feature and fit a linear model on these extended features.\n",
    "\n",
    "$$y = \\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+\\ldots+\\beta_nx^n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJZbh38hdsOk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PolynomialFeatures(degree=2),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "\n",
    "default_num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PolynomialFeatures(degree=2),\n",
    "    StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preprocessing_poly = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline, [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline, [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline, [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"population\", \"households\", \"median_income\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),],\n",
    "    remainder=default_num_pipeline)\n",
    "\n",
    "preprocessing_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdZmFL_xdsOk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg = make_pipeline(preprocessing_poly, LinearRegression())\n",
    "poly_rmses = cross_validate(poly_reg,\n",
    "                            housing,\n",
    "                            housing_labels,\n",
    "                            scoring=\"neg_root_mean_squared_error\",\n",
    "                            return_train_score=True,\n",
    "                            cv=5)\n",
    "\n",
    "print(\"rmses (train): \", -poly_rmses['train_score'])\n",
    "print(\"average train rmse: \", -poly_rmses['train_score'].mean())\n",
    "print(\"rmses (val):\", -poly_rmses['test_score'])\n",
    "print(\"average val rmse:\", -poly_rmses['test_score'].mean())\n",
    "\n",
    "print('diff:', abs(poly_rmses['test_score'].mean()-poly_rmses['train_score'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsqt-F4JdsOk"
   },
   "source": [
    "We noticed that the mean train rmse has improved but the validation is still much worse than training rmse. We may want to try a regularized Linear regressor `Ridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMN-MnQ6dsOk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = make_pipeline(preprocessing_poly, Ridge())\n",
    "\n",
    "ridge_rmses = cross_validate(ridge_reg,\n",
    "                             housing,\n",
    "                             housing_labels,\n",
    "                             scoring=\"neg_root_mean_squared_error\",\n",
    "                             return_train_score=True,\n",
    "                             cv=5)\n",
    "\n",
    "print(\"rmses (train): \", -ridge_rmses['train_score'])\n",
    "print(\"average train rmse: \", -ridge_rmses['train_score'].mean())\n",
    "print(\"rmses (val):\", -ridge_rmses['test_score'])\n",
    "print(\"average val rmse:\", -ridge_rmses['test_score'].mean())\n",
    "print('diff:', abs(ridge_rmses['test_score'].mean()-ridge_rmses['train_score'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aesgVLKEdsOk"
   },
   "source": [
    "With regularization, our model's bias has increased but the variance has decreased slightly. Our validation rmse is now closer to our train rmse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Yxb4D1dsOl"
   },
   "source": [
    "We can also try other more sophisticated algorithms such as RandomForestRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCEMVX8cdsOl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = make_pipeline(preprocessing, RandomForestRegressor())\n",
    "\n",
    "forest_rmses = cross_validate(forest_reg,\n",
    "                             housing,\n",
    "                             housing_labels,\n",
    "                             scoring=\"neg_root_mean_squared_error\",\n",
    "                             return_train_score=True,\n",
    "                             cv=3)\n",
    "\n",
    "print(\"rmses (train): \", -forest_rmses['train_score'])\n",
    "print(\"average train rmse: \", -forest_rmses['train_score'].mean())\n",
    "print(\"rmses (val):\", -forest_rmses['test_score'])\n",
    "print(\"average val rmse:\", -forest_rmses['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3fSUcHXdsOl"
   },
   "source": [
    "The training rmse is a big improvement from Linear Regression model, however, we see that there is quite a fair bit of overfitting here. Some regularization will be helpful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aziQXl2JJ6oF"
   },
   "source": [
    "## Model Fine Tuning\n",
    "\n",
    "Now that we have our first model, we can try to improve the model by adjusting some of the hyper-parameters. This process is called fine-tuning. One way to fine tune the model is to use Scikit-Learn's `GridSearchCV` to evaluate all the possible combiniations of hyperparameter values that you want it to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D-gk857J_jL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_poly),\n",
    "    (\"ridge\", Ridge()),\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = [{'ridge__alpha': [0.0001, 0.001, 0.01, 1, 10]}]\n",
    "\n",
    "grid_search = GridSearchCV(full_pipeline,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rn9_gfawdsOl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('best params:', grid_search.best_params_)\n",
    "print('best score:', -(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXByYOTGdsOl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "# # extra code – these few lines of code just make the DataFrame look nicer\n",
    "cv_res = cv_res[[\"param_ridge__alpha\",\n",
    "                 \"split0_test_score\",\n",
    "                 \"split1_test_score\",\n",
    "                 \"split2_test_score\",\n",
    "                 \"split3_test_score\",\n",
    "                 \"split4_test_score\",\n",
    "                 \"mean_test_score\"]]\n",
    "score_cols = [\"split0\", \"split1\", \"split2\", \"split3\", \"split4\", \"mean_test_rmse\"]\n",
    "cv_res.columns = [\"alpha\"] + score_cols\n",
    "cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n",
    "\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XElKJxFgbPgJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge_reg = make_pipeline(preprocessing_poly, Ridge(alpha=0.0001))\n",
    "final_model = ridge_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vydyp1Mk7xFY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "housing_predictions = ridge_reg.predict(housing)\n",
    "mse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEhQAYXQNcM5"
   },
   "source": [
    "### Evaluate System on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSGFdziWdsOm"
   },
   "source": [
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. You are ready to evaluate the final model on\n",
    "the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hPyYHOkwITQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "final_predictions = final_model.predict(X_test)\n",
    "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbx_Ce0DdsOn"
   },
   "source": [
    "## Deploy your model\n",
    "\n",
    "We now need to get our model ready for deployment to production environment. The most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the joblib library like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x52b3-4UdsOn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(final_model, \"my_california_housing_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fESSlnjdsOn"
   },
   "source": [
    "Once your model is transferred to production, we can load it and use it. For this we must first import any custom classes and functions the model relies on (which means transferring the code to production), then load the model using joblib and use it to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTPW7JGbdsOn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"] # feature names out\n",
    "\n",
    "final_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjAIVdKT7xFa"
   },
   "source": [
    "Typicall we will have some kind of flask web service to serve the model. Here for simplicity, we just try out our model using some sample test data in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVrOAf797xFa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data = X_test.iloc[-5:]  # pretend these are new districts\n",
    "predictions = np.round(final_model_reloaded.predict(new_data))\n",
    "actual = y_test.iloc[-5:].values\n",
    "print(predictions)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY-0G4q37xFb"
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id-5i6l07xFb"
   },
   "source": [
    "In the early [section](#Question-2), we noticed some data quirks at median housing values of \\\\$500,0001, \\\\$450,000 and \\\\$350,000. Try to remove these data quirks to see if you are able to produce a more accurate model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7b45d97-d213-4bff-a39a-fb973be3d1c2",
    "_uuid": "40baadcf98e87b9fa263ec2c80804a6298d6472e"
   },
   "source": [
    "# Fraud Detection \n",
    "\n",
    "In this exercise, we will build a financial fraud detection model. The model is a binary classifier that classifies a transaction as non-fraud (negative case) and fraud (positive case).\n",
    "\n",
    "There is a lack of publicly available datasets on financial services and specially in the emerging mobile money transactions domain. We will be using a sythetic dataset called PaySim. PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. You can find out more how the data is generate from the [paper](http://www.msc-les.org/proceedings/emss/2016/EMSS2016_249.pdf).\n",
    "\n",
    "Here are the description of the different columns of the PaySim dataset: \n",
    "\n",
    "|Field|Description|\n",
    "|-----|-----|\n",
    "|step|Maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).|\n",
    "|type|CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER|\n",
    "|amount|Amount of the transaction in local currency|\n",
    "|nameOrig|Customer who started the transaction|\n",
    "|oldbalanceOrg|Initial balance before the transaction|\n",
    "|newbalanceOrig|New balance after the transaction|\n",
    "|nameDest|Customer who is the recipient of the transaction|\n",
    "|oldbalanceDest|Initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants)|\n",
    "|newbalanceDest|New balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants)|\n",
    "|isFlaggedFraud|The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction|\n",
    "|isFraud|This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aae6646e-bcaa-4afb-85bc-2f67e314d5b0",
    "_uuid": "d821b548edc31ad27967b8419af17bfa4f0bae48"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "03afcaca-4105-4c95-9a38-9a922f592814",
    "_uuid": "f936a10915bc042e678e1148923e493f92b518c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance, to_graphviz\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9bc1119f-d883-4cc6-b569-a707359abe01",
    "_uuid": "b7a3f0c8714f9e3d78a898a014f5ad81ecee1063",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Fraud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da58e56c-7ba7-46b2-8b84-d1bcea8c7e42",
    "_uuid": "435ddb566ca27f5ce720f0fcb52e62e99dc829dd"
   },
   "source": [
    "For consistency, let's correct spelling of original column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9dc91648-ab99-44c3-a1f1-bea967df390f",
    "_uuid": "6e1110dd564e4a38eb9d8e83c4575dcc3d26abe8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \\\n",
    "                        'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51d76f51-53ed-4ba5-ae39-588909fcb531",
    "_uuid": "d0987a74164e0d432df7291eb91338a6a6b17fe2"
   },
   "source": [
    "Let's check for any missing values. It turns out there are no obvious missing values but, as we will see below, value 0 may be used as a proxy if no data is available (e.g. those in the newBalanaceOrig, newBalanceDest, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f073780-2735-4481-a80c-87ed3cfc9d6a",
    "_uuid": "4409f927b742283f1024982d00585deae5e57976"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "In this section, we will do some data-wrangling to gain more insights into the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Let's find out how many different types of transactions first. How many types of transactions are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## complete the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cff74f1a-5121-44c2-a6fe-6d69fd904a45",
    "_uuid": "856038be1f75ffd6e0b132c80eac2a35bb36f9d5"
   },
   "source": [
    "#### Which types of transactions are fraudulent? \n",
    "\n",
    "We would like to find out which type of transactions are most fraudulent? We find that of the various types of transactions, fraud occurs only in two of them:\n",
    "'TRANSFER' where money is sent to a customer / fraudster and 'CASH_OUT' where money is sent to a merchant who pays the customer / fraudster in cash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isFraud == 1].type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the percentage of fraudulent transactions that belong to TRANSFER type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_fraudulent_xfer = len(df.loc[ (df.isFraud == 1) & (df.type == 'TRANSFER')])\n",
    "percentage_fraudulent_xfer = num_fraudulent_xfer / len(df.loc[df.isFraud == 1])\n",
    "print(f'number of fraudulent TRANSFER = {num_fraudulent_xfer}')\n",
    "print(f'percentage of fraudulent TRANSFER = {percentage_fraudulent_xfer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Find out how many fraudulent CASH_OUT?  \n",
    "2. Find out what is the percentage of fraudulent transactions is of CASH_OUT type.\n",
    "3. What do you observe? \n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "<br/>\n",
    "    \n",
    "```python \n",
    "num_fraudulent_cash_out = len(df.loc[ (df.isFraud == 1) & (df.type == 'CASH_OUT')])\n",
    "percentage_fraudulent_cash_out = num_fraudulent_cash_out / len(df.loc[df.type == 'CASH_OUT'])\n",
    "print(f'number of fraudulent CASH_OUT = {num_fraudulent_cash_out}')\n",
    "print(f'percentage of fraudulent CASH_OUT = {percentage_fraudulent_cash_out}')\n",
    "```\n",
    "<br/>\n",
    "We observe that the number of fraudulent TRANSFERs is almost equals the number of fraudulent CASH_OUTs These observations appear to bear out the description provided on Kaggle that the modus operandi of fraudulent transactions in  this dataset is committed by first transferring out funds to another account which subsequently cashes it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## complete the code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there account labels common to fraudulent TRANSFERs and CASH_OUTs?\n",
    "\n",
    "From the data description, the modus operandi for committing fraud involves first making a TRANSFER to a (fraudulent) account which in turn conducts a CASH_OUT. CASH_OUT involves transacting with a merchant who pays out cash. Thus, within this two-step process, the fraudulent account would be both, the destination in a TRANSFER and the originator in a CASH_OUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba9c708c-2f0a-4598-9563-7abaf50120f9",
    "_uuid": "8598fa5d577c02ff00638da238a59663d8259a7d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfFraudTransfer = df.loc[(df.isFraud == 1) & (df.type == 'TRANSFER')]\n",
    "dfFraudCashout = df.loc[(df.isFraud == 1) & (df.type == 'CASH_OUT')]\n",
    "\n",
    "print('\\nOf all fraudulent transactions, destinations for TRANSFERS that are also originators for CASH_OUTs? {}'.format(\\\n",
    "(dfFraudTransfer.nameDest.isin(dfFraudCashout.nameOrig)).any())) # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5ddbc33-3da5-4095-b4bc-06ee673afb5e",
    "_uuid": "77dfff4adb09874c1bc4b3e775a60bb98beeba93"
   },
   "source": [
    "However, our analysis above showed there are no  such common accounts among fraudulent transactions. Thus, the data is not imprinted with the expected modus-operandi. We can therefore drop the nameDest and nameOrig from the features used for modelling later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Are the destination accounts with zero balances before and after non-zero amount is transacted normal?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has several transactions with zero balances in the destination account both before and after a non-zero amount is transacted. \n",
    "\n",
    "Let's find out the how many of these transactions, of type TRANSFER/CASH_OUT,  are actually fraudulent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfFraudTransferCashOut = df.loc[(df.isFraud == 1) & ((df.type == 'TRANSFER') | (df.type == 'CASH_OUT'))]\n",
    "dfNonFraudTransferCashOut = df.loc[(df.isFraud == 0) & ((df.type == 'TRANSFER') | (df.type == 'CASH_OUT'))]\n",
    "\n",
    "print('\\nThe fraction of fraudulent transactions with \\'oldBalanceDest\\' = \\\n",
    "\\'newBalanceDest\\' = 0 although the transacted \\'amount\\' is non-zero is: {}'.\\\n",
    "format(len(dfFraudTransferCashOut.loc[(dfFraudTransferCashOut.oldBalanceDest == 0) & \\\n",
    "(dfFraudTransferCashOut.newBalanceDest == 0) & (dfFraudTransferCashOut.amount != 0)]) / (1.0 * len(dfFraudTransferCashOut))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Now find out how many of these transactions (of type TRANSFER/CASH_OUT) are genuine (non-fraudulent) transaction. \n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "<br/>\n",
    "\n",
    "```python \n",
    "print('\\nThe fraction of genuine transactions with \\'oldBalanceDest\\' = \\\n",
    "newBalanceDest\\' = 0 although the transacted \\'amount\\' is non-zero is: {}'.\\\n",
    "format(len(dfNonFraudTransferCashOut.loc[(dfNonFraudTransferCashOut.oldBalanceDest == 0) & \\\n",
    "(dfNonFraudTransferCashOut.newBalanceDest == 0) & (dfNonFraudTransferCashOut.amount != 0 )]) / (1.0 * len(dfNonFraudTransferCashOut))))\n",
    "```\n",
    "<br/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Complete the code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The fraction of such transactions, where zero likely denotes a missing value, is much larger in fraudulent (50%) compared to genuine transactions (0.06%). This shows that a 0 in the oldBalanceDest and newBalanceDest is a strong indicator of fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The fraction of fraudulent transactions with 'oldBalanceOrig' = 'newBalanceOrig' = 0 although the transacted 'amount' is non-zero is: {}\".\n",
    "      format(len(dfFraudTransferCashOut.loc[(dfFraudTransferCashOut.oldBalanceOrig == 0) & \n",
    "                                            (dfFraudTransferCashOut.newBalanceOrig == 0) & \n",
    "                                            (dfFraudTransferCashOut.amount != 0)]) / (1.0 * len(dfFraudTransferCashOut))))\n",
    "\n",
    "print(\"The fraction of genuine transactions with 'oldBalanceOrig' = newBalanceOrig' = 0 although the transacted 'amount' is non-zero is: {}\".\n",
    "      format(len(dfNonFraudTransferCashOut.loc[(dfNonFraudTransferCashOut.oldBalanceOrig == 0) & \n",
    "                                               (dfNonFraudTransferCashOut.newBalanceOrig == 0) & \n",
    "                                               (dfNonFraudTransferCashOut.amount != 0 )]) / (1.0 * len(dfNonFraudTransferCashOut))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f02a6bb-08db-41b6-b0d1-2076e49a47e8",
    "_uuid": "b8e9aab218faffb19fa309eb3ef754d2cc58c4a4"
   },
   "source": [
    "### Feature-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee52641e-2440-4a48-b312-77369d9f2841",
    "_uuid": "f31f4503e62234aedc3782a64374207a1a6366a7"
   },
   "source": [
    "Motivated by the possibility of zero-balances serving to differentiate between fraudulent and genuine transactions, we create 2 new features (columns) recording errors in the  originating and destination accounts for each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "51aac74d-bdd1-4eeb-8819-3fc0ec58ea20",
    "_uuid": "7ab7061e23ae18e435fd88b9f1fb418f88865d96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['errorBalanceOrig'] = df.newBalanceOrig + df.amount - df.oldBalanceOrig\n",
    "df['errorBalanceDest'] = df.oldBalanceDest + df.amount - df.newBalanceDest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of features to the target label\n",
    "\n",
    "Let's find out the correlation of each of our numerical features with the target label, for all TRANSFER/CASH_OUT transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfTransferCashOut = df[ (df.type == 'TRANSFER') | (df.type == 'CASH_OUT') ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = dfTransferCashOut.corr()\n",
    "# corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix['isFraud'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cdd94133-5730-4f31-9a7e-c23c1322edb2",
    "_uuid": "a5b91ecc33d7f51bec7e5e357edef789e2459d12"
   },
   "source": [
    "From the exploratory data analysis (EDA), we know that fraud only occurs in 'TRANSFER's and 'CASH_OUT's. So we create a train/test set only from those transaction. Also, we will drop the nameOrig and nameDest, as our EDA shows that they are not relevant in predicting if a transaction is fraud or not.  We also need to convert the TRANSFER and CASHOUT to a numeric value instead.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. Create a dataframe that consists of TRANSFER/CASH_OUT transactions only\n",
    "2. Drop the following features 'nameOrig', 'nameDest' \n",
    "3. Map the type TRANSFER to numeric value 0, and CASH_OUT to numeric value 1\n",
    "4. create features (X), and labels (y) \n",
    "5. create a stratified train/test split of 80:20 ratio \n",
    "\n",
    "<br/>\n",
    "<details><summary>Click here for solution</summary>\n",
    "\n",
    "<br/>\n",
    "\n",
    "```python\n",
    "#1. Create a dataframe that consists of TRANSFER/CASH_OUT transactions only\n",
    "\n",
    "dfTransferCashOut = df.loc[(df.type == 'TRANSFER') | (df.type == 'CASH_OUT')]\n",
    "\n",
    "#2. Drop the following features 'nameOrig', 'nameDest'\n",
    "dfTransferCashOut = dfTransferCashOut.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)\n",
    "\n",
    "#3. Map the type TRANSFER to numeric value 0, and CASH_OUT to numeric value 1\n",
    "dfTransferCashOut['type'] = dfTransferCashOut['type'].apply(lambda x: 0 if x == 'TRANSFER' else 1)\n",
    "\n",
    "#4. create features (X) and labels (y) \n",
    "y = dfTransferCashOut['isFraud'] \n",
    "X = dfTransferCashOut.drop('isFraud', axis=1)\n",
    "\n",
    "# 5. Create train/test split of 80:20 ratio, in stratified  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state = 49)\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc50e596-b0f3-4e63-877d-d506e2528cd1",
    "_uuid": "42807f63bb86782daedce8dfa7ba1385458c84c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Complete the code \n",
    "\n",
    "# 1. Create a dataframe that consists of TRANSFER/CASH_OUT transactions only\n",
    "\n",
    "\n",
    "\n",
    "# 2. Drop the following features 'nameOrig', 'nameDest'\n",
    "\n",
    "\n",
    "# 3. Map the type TRANSFER to numeric value 0, and CASH_OUT to numeric value 1\n",
    "\n",
    "\n",
    "\n",
    "# 4. create features (X) and labels (y) \n",
    "\n",
    "\n",
    "\n",
    "# 5. Create train/test split of 80:20 ratio \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from below, the data is highly imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1de6d509-4ea6-4aec-a465-b06bc2beafa8",
    "_uuid": "3696671c29b0d206491e2405650cd0830a86997c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is highly imbalanced, we will use the Area under Precision/Recall Curve (Average Precision) as metrics to measure the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "\n",
    "Let's train a Logistic Regressor to classify the fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c4cd5e93-8038-4cba-a48d-f3e227db1f54",
    "_uuid": "ed0275e8371cc951382844a61def5a9c994e4d78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_probs = lr_clf.predict_proba(X_test)\n",
    "print('AUPRC for test set = {}'.format(average_precision_score(y_test, test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_probs = lr_clf.predict_proba(X_train)\n",
    "print('AUPRC for train set = {}'.format(average_precision_score(y_train, train_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = lr_clf.predict(X_test)\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is not great. Our recall/precison for fraud class is rather disappointing.  Let's apply higheer weightage to the minority class and try again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(class_weight='balanced')\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_scores = lr_clf.decision_function(X_train)\n",
    "print('AUPRC for train set = {}'.format(average_precision_score(y_train, train_scores)))\n",
    "test_scores = lr_clf.decision_function(X_test)\n",
    "print('AUPRC for test set = {}'.format(average_precision_score(y_test, test_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = lr_clf.predict(X_test)\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "What do you observe about the precision/recall of the fraud class? \n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "    \n",
    "It seems that by placing more weights on 'fraud' class, we have managed to improve the recall rate for 'Fraud' but also increase the false positive (lower precision). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling \n",
    "\n",
    "It is difficult for our model to learn from a highly skewed dataset such as this, as there are simply too few fraud samples (minority class) compared to non-fraud (majority class). We can use oversampling technique to create more samples for minority class to make it a more balanced dataset. SMOTE (and it's variants are one such technique). \n",
    "\n",
    "Note that we should only perform oversampling on the train split only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#over_sample = SMOTE(sampling_strategy='auto')\n",
    "over_sample = BorderlineSMOTE(sampling_strategy='auto')\n",
    "X_train_oversampled,y_train_oversampled = over_sample.fit_resample(X_train,y_train)\n",
    "print(y_train_oversampled.value_counts()) #resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_clf_oversampled = LogisticRegression()\n",
    "lr_clf_oversampled.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Model Performance \n",
    "\n",
    "As mentioned before, for a skew dataset, accuracy is not a good metrics to use.  Let's see what is our accuracy score first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_preds = lr_clf_oversampled.predict(X_test)\n",
    "accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy looks very good. But this is misleading. We can find better insights by looking at the performance of each class (positive/negative) with classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under Precision Recall Curve (Average Precision)\n",
    "\n",
    "Let's calculate the Area under Precision Recall curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_probs = lr_clf_oversampled.predict_proba(X_train)\n",
    "print('AUPRC for train set = {}'.format(average_precision_score(y_train, train_probs[:,1])))\n",
    "test_probs = lr_clf_oversampled.predict_proba(X_test)\n",
    "print('AUPRC for test set = {}'.format(average_precision_score(y_test, test_probs[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AUPRC seems to improve with oversampling. However, if we look at the classification report for the positive and negative class, we see that our precision with positive class (i.e. the fraud case) is extremely low, i.e. a lot of false positives.  The `predict()` function uses a default threshold of 0.5 (probability) to decide if something is a positive or negative class. We can adjust this threshold to trade-off a bit of recall with precision, as in the following \"Precision Recall Trade-off\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Recall Trade-off "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the precision recall curve for visualization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_test, test_probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_predictions(\n",
    "    y_test, test_probs[:,1], name=\"LogisticRegression\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we want to get a higher precision and we are willing to trade-off for a lower recall, e.g. 0.65. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_65_recall = thresholds[np.argmin(recalls >= 0.65)]\n",
    "threshold_65_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the threshold is now at more than 0.99, i.e. we will only classify a case as \"fraud\" if the classifier is 99% sure that this is fraud. Now we can use this threshold for deciding if a prediction is positive (fraud) or negative (non-fraud) by checking if it is above or below this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_pred_65 = (test_probs[:,1] >= threshold_65_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred_65))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the classification report, you can see that we achieve a higher precision of 0.58 if we are willing to lower our recall rate to 0.65. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Assuming you want to have a higher recall rate of 0.75 instead of 0.65.  What is the expected precision for positive class? \n",
    "Write code to print the classification report based on this new criteria. \n",
    "\n",
    "<details><summary>Click here for solution</summary>\n",
    "    \n",
    "```python \n",
    "threshold_75_recall = thresholds[np.argmin(recalls >= 0.75)]\n",
    "threshold_75_recall\n",
    "y_test_pred_75 = (test_probs[:,1] >= threshold_75_recall)\n",
    "print(classification_report(y_test, y_test_pred_75))\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Complete the code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our linear model underfits quite badly with the data. Let's try a more complex ensemble model with boosting algorithms. In this case we will use a very fast boosting algorithm called lightGBM (you can try other boosting algorithm such as XGBoost)\n",
    "\n",
    "Note: We did not cover this algorithm in the lecture. but we are using it here for comparison only. To learn more about lightGBM, you can refer to the [lightGBM website](https://github.com/microsoft/LightGBM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgbm_clf = lgb.LGBMClassifier(num_leaves=30, learning_rate=0.05, n_estimators=30) \n",
    "lgbm_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_probs = lgbm_clf.predict_proba(X_train)\n",
    "print('AUPRC for train set = {}'.format(average_precision_score(y_train, train_probs[:, 1])))\n",
    "test_probs = lgbm_clf.predict_proba(X_test)\n",
    "print('AUPRC for test set = {}'.format(average_precision_score(y_test, test_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = lgbm_clf.predict(X_test)\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that we achieve an almost perfect classifier!  Normally boosting classifier is very effective in dealing with imbalanced dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LightGBM also allows us to plot the importance of various features used in the model. The feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgbm_clf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
